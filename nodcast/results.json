[{"id": "339b5aff-1b25-39bb-9d23-c6a10e743b05", "title": "On the Binding Problem in Artificial Neural Networks", "score": 0.0, "url": "https://arxiv.org/abs/2012.05208", "pdfUrl": "https://arxiv.org/pdf/2012.05208.pdf", "bibTexUrl": null, "authors": ["Klaus Greff", "Sjoerd van Steenkiste", "J\u00fcrgen Schmidhuber"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Contemporary neural networks still fall short of human-level generalization, which extends far beyond our direct experiences. In this paper, we argue that the underlying cause for this shortcoming is their inability to dynamically and flexibly bind information that is distributed throughout the network. This binding problem affects their capacity to acquire a compositional understanding of the world in terms of symbol-like entities (like objects), which is crucial for generalizing in predictable and systematic ways. To address this issue, we propose a unifying framework that revolves around forming meaningful entities from unstructured sensory inputs (segregation), maintaining this separation of information at a representational level (representation), and using these entities to construct new inferences, predictions, and behaviors (composition)....", "startOffset": 0, "endOffset": 861}], "summary": false}, {"title": "Introduction", "figure": null, "fragments": [{"end_offset": 185, "text": "They require large amounts of data, struggle with transfer to novel tasks, and are fragile under distributional shift.", "start_offset": 67}, {"end_offset": 526, "text": "One explanation for this discrepancy is that neural networks mostly learn about surface statistics in place of the underlying concepts, which prevents them from generalizing systematically.", "start_offset": 337}, {"end_offset": 1010, "text": "In this paper, we will view the inability of contemporary neural networks to effectively form, represent, and relate symbol-like entities, as the root cause of this problem. This emphasis on symbolic reasoning reflects a common sentiment within the community and others have advocated similar perspectives .", "start_offset": 644}, {"end_offset": 1630, "text": "Meanwhile, the persistent failure of neural networks to generalize systematically is evidence that neural networks do not acquire the ability to process information symbolically, simply as a byproduct of learning.", "start_offset": 1417}, {"end_offset": 2351, "text": "In contrast, we believe that these problems stem from a deeper underlying cause that is best addressed directly from within the framework of connectionism. In this work, we argue that this underlying cause is the binding problem: The inability of existing neural networks to dynamically and flexibly bind information that is distributed throughout the network.", "start_offset": 1991}, {"end_offset": 2841, "text": "Each of these aspects relates to a wealth of research in neuroscience and cognitive psychology, where the binding problem has been extensively studied in the context of the human brain.", "start_offset": 2656}, {"end_offset": 3521, "text": "Our analysis provides a starting point for identifying the right combination of inductive biases to enable neural networks to process information symbolically and generalize more systematically. In our view, integrating symbolic processing into neural networks is of fundamental importance for realizing human-level AI, and will require a joint community effort to resolve.", "start_offset": 3148}], "summary": true}, {"title": "The Binding Problem", "figure": 15, "fragments": [{"end_offset": 377, "text": "We start our discussion by reviewing the importance of symbols as units of computation and highlight several symptoms that point to the lack of emergent symbolic processing in existing neural networks. We argue that this is a major obstacle for achieving human-level generalization, and posit that the binding problem in connectionism is the underlying cause for this weakness.", "start_offset": 0}, {"end_offset": 858, "text": "We are able to reason causally about unfamiliar scenes, understand novel sentences with ease, and use models and analogies to make predictions about entities far outside the scope of everyday reality, like atoms, and galaxies.", "start_offset": 632}, {"end_offset": 1215, "text": "The best explanation for this remarkable cognitive capacity revolves around symbolic thought: the ability to form, manipulate, and relate mental entities that can be processed like symbols .", "start_offset": 1008}, {"end_offset": 4263, "text": "Rather than relying on hand-crafted symbols and rules, connectionist approaches such as neural networks focus on learning suitable distributed representations directly from low-level sensory data.", "start_offset": 4067}, {"end_offset": 5820, "text": "On the other hand, it has become increasingly evident that neural networks fall short in many aspects of human-level generalization, including those that symbolic approaches exhibit by design.", "start_offset": 5628}, {"end_offset": 7740, "text": "A hybrid approach that combines the seemingly complementary strengths of neural networks and symbolic approaches may help address these issues, and several variations have been explored .", "start_offset": 7528}, {"end_offset": 9894, "text": "We claim that there exists an underlying cause for the lack of emergent symbolic processing in neural networks, which we refer to as the binding problem.", "start_offset": 9741}, {"end_offset": 10904, "text": "It includes perceptual binding problems such as visual binding , auditory binding , binding across time , cross-modal binding , motor-behavior , and sensorimotor binding .", "start_offset": 10564}, {"end_offset": 15409, "text": "It is concerned with the information binding required for dynamically creating object representations, as well as the characteristics of objects as modular building blocks for guiding this process.", "start_offset": 15212}], "summary": true}, {"title": "Representation", "figure": 16, "fragments": [{"end_offset": 300, "text": "In this section, we look at the binding problem from the perspective of representation. We have argued that, to take advantage of symbolic processing, neural networks require some form of object representations that combine the richness of neural representations with the compositionality of symbols.", "start_offset": 0}, {"end_offset": 781, "text": "This has direct consequences for the representational format and its underlying dynamics. Consider for example Figure 4, where you are able to distinguish between five different objects. You can readily describe each object in terms of its shape, color, material, and other properties, despite most likely never having encountered them before.", "start_offset": 438}, {"end_offset": 2718, "text": "To support the construction of structured models, object representations need to act as modular building blocks.", "start_offset": 2606}, {"end_offset": 6407, "text": "An object representation requires ongoing updates across time for a number of reasons: Firstly, with objects constantly moving and transforming in the real world, their corresponding representations need adjustments to remain accurate.", "start_offset": 6172}, {"end_offset": 7597, "text": "The reliability of this foundation is especially important for more abstract computations to which object representations provide the only connection to the world.", "start_offset": 7434}, {"end_offset": 12194, "text": "With weights typically being shared across steps, sequential slots naturally share a common format and unlike other slot-based representations can dynamically adjust their representational capacity.", "start_offset": 11990}, {"end_offset": 15873, "text": "Object representations based on augmentation will trivially be in a common format, although extracting information about individual objects now requires first processing the grouping information.", "start_offset": 15678}], "summary": true}, {"title": "Segregation", "figure": 0, "fragments": [{"end_offset": 131, "text": "In this section, we look at the binding problem from the perspective of segregation: the process of forming object representations.", "start_offset": 0}, {"end_offset": 739, "text": "Humans effortlessly perceive the world in terms of objects, yet this process of perceptual organization is surprisingly intricate . Even for everyday objects like a mirror, a river, or a house, it is difficult to formulate precise boundaries or a definition that generalizes across multiple different contexts.", "start_offset": 413}, {"end_offset": 3908, "text": "By organizing information in this way, objects are expected to capture information that is due to independent causes, which matches our intuitive notion of objects in the real world .", "start_offset": 3698}, {"end_offset": 5994, "text": "Here, parts are themselves objects, which are the result of dynamically separating information into object representations .", "start_offset": 5857}, {"end_offset": 7242, "text": "This implies that objects can be simultaneously grounded in sensory information from multiple domains, which may help resolve ambiguities .", "start_offset": 7055}, {"end_offset": 10850, "text": "Hence it is important that the outcome of the segregation process can be steered towards the most useful decomposition, based on contextual information.", "start_offset": 10698}, {"end_offset": 18166, "text": "Approximate instance segments can also be obtained as a by-product of performing some other task, such as learning to interpolate between multiple images or minimizing mutual information between image segments .", "start_offset": 17908}, {"end_offset": 23609, "text": "For example, in MONet , GENESIS , and ECON a recurrent neural network is trained to directly support the learning of object representations by outputting a mask that focuses on different objects at each step14.", "start_offset": 23326}, {"end_offset": 24812, "text": "This then enables inference to go beyond low-level similarities or spatial proximity, and recover object representation based on their high-level structure as implied by the model.", "start_offset": 24618}], "summary": true}, {"title": "Composition", "figure": 7, "fragments": [{"end_offset": 513, "text": "In this section, we look at the binding problem from the perspective of composition: building structured models of the world that are compositional. Here we encounter the need for variable binding: the ability to combine object representations and relations without losing their integrity as constituents . As we have seen in Section 2, compositionality is a core aspect of human cognition and underlies our ability to understand novel situations in terms of existing knowledge.", "start_offset": 0}, {"end_offset": 2522, "text": "To implement structured models, a neural network must organize its computations to reflect the desired structure in terms of objects and their relations.", "start_offset": 2369}, {"end_offset": 8310, "text": "Analogously, combinatorial entailment is used to derive new relations between two objects, based on their relations with a shared third object, e.g.", "start_offset": 8162}, {"end_offset": 9664, "text": "The appropriate structure for a model depends on the task and context, and should therefore be dynamically inferred by the neural network to focus only on relevant interactions between the objects. Likewise, it is important to consider the computational interactions between relations and object representations, in order to make use of the inferred structure for prediction and behavior.", "start_offset": 9276}, {"end_offset": 10302, "text": "More generally, relational responding of this kind may involve evaluating multiple relations between objects and combining information across different relational frames.", "start_offset": 10122}, {"end_offset": 15906, "text": "In the context of composition, nodes correspond to object representations and edges to relations, which together form the structure, i.e. using variable binding at the architectural level.", "start_offset": 15709}, {"end_offset": 16571, "text": "In general, the local information processing in a GNN ensures that information affects the object representations in a way that follows the dependency structure implied by the relations .", "start_offset": 16361}], "summary": true}, {"title": "Insights from Related Disciplines", "figure": 11, "fragments": [{"end_offset": 708, "text": "Object perception and the symbolic nature of human cognition have been studied from various angles in Neuroscience, Psychology, Linguistics, and Philosophy. These complementary perspectives provide valuable inspiration for addressing the binding problem and we have frequently drawn upon their insights throughout this survey. While an exhaustive overview is outside the scope of this survey, we provide a brief discussion of the areas that were most influential to the development of the conceptual framework presented here. These fields have a lot more to offer and we encourage the reader to further explore this literature, for example by using the pointers and connections provided here as entry-points.", "start_offset": 0}, {"end_offset": 1640, "text": "The concept of a Gestalt closely resembles our notion of objects and Gestalt Psychology was arguably the first systematic investigation of human object perception .", "start_offset": 1436}, {"end_offset": 6288, "text": "Another important empirical finding occurs when attention is overloaded , which sometimes causes humans to perceive illusory conjunctions: illusory objects that are the result of wrongly combining features from other objects .", "start_offset": 6000}, {"end_offset": 7740, "text": "FIT has been a highly influential model of human visual attention and could serve as further inspiration for attention-based segregation.", "start_offset": 7603}, {"end_offset": 9667, "text": "In this case, neurons whose activation encodes features of one object are expected to fire in synchrony , while neurons encoding features belonging to different objects would be out of phase with each other .", "start_offset": 9386}, {"end_offset": 11240, "text": "Relational Frame Theory is a theory of behavioral psychology about relating and offers interesting insights about composing and systematic generalization in humans.", "start_offset": 10967}, {"end_offset": 16985, "text": "The principle of compositionality is thus an inference to the best explanation because it is difficult to imagine language being productive, systematic, and computationally efficient without its semantics being somehow compositional in the above sense.", "start_offset": 16733}], "summary": true}, {"title": "Discussion", "figure": null, "fragments": [{"end_offset": 872, "text": "The ultimate motivation of this work is to address the shortcomings of neural networks at human-level generalization. To this end, we have developed a conceptual framework centered around compositionality and the binding problem. Our analysis identifies the binding problem as the primary cause for these shortcomings, and thereby paves the way for a single unified solution. It rests on several assumptions regarding the nature and importance of objects and the learning capabilities of neural networks. In the following, we explicate several of these assumptions and use them to contrast with other conceptual frameworks aimed at addressing human-level generalization. One of the main assumptions behind our work is that objects are key to compositionality and that the latter plays a fundamental role in generalizing more systematically.", "start_offset": 0}, {"end_offset": 1613, "text": "Importantly, we assume that objects at any level of abstraction are essentially the result of decomposing a given problem into modular building blocks, and thus share the same underlying computational mechanisms.", "start_offset": 1401}, {"end_offset": 2608, "text": "Here we have proposed to address this problem by incorporating a small set of inductive biases to enable neural networks to process information more symbolically, while also preserving the crucial benefits of end-to-end learning .", "start_offset": 2364}, {"end_offset": 3418, "text": "Hence, Lake et al. advocate the use of specialized inductive biases inspired by cognitive psychology, and using neural networks as a means for implementing fast inference within the context of larger structured models.", "start_offset": 3193}, {"end_offset": 4177, "text": "Compared to Lake et al. our focus on integrating high-level reasoning with low-level perception in neural networks puts a lot more emphasis on symbol grounding and the associated problem of segregation.", "start_offset": 3968}], "summary": true}, {"title": "Conclusion", "figure": null, "fragments": [{"end_offset": 433, "text": "Humans understand the world in terms of abstract entities, like objects, whose underlying compositionality allows us to generalize far beyond our direct experiences. At present, neural networks are unable to generalize in the same way. In this paper, we have argued that this limitation is largely due to the binding problem, which impairs the ability of neural networks to effectively incorporate symbol-like object representations.", "start_offset": 0}, {"end_offset": 1234, "text": "Based on this division, we have offered a conceptual framework for addressing the lack of symbolic reasoning capabilities in neural networks that is believed to be the root cause for their lack of systematic generalization.", "start_offset": 1011}, {"end_offset": 2357, "text": "For a new situation, the most useful decomposition in terms of objects depends not only on the task, but also on the abstractions, relations, and general problem-solving capabilities available to the entire system.", "start_offset": 2101}, {"end_offset": 2979, "text": "Addressing these open problems may pave the way for an integrated system that can learn to dynamically construct structured models for prediction, inference, and behavior in a way that generalizes similarly to humans.", "start_offset": 2762}, {"end_offset": 4982, "text": "Concerning the binding problem, we focused primarily on encoding information about objects in working memory, although similar problems arise in the context of long-term memory.", "start_offset": 4805}, {"end_offset": 6266, "text": "Finally, a comprehensive treatment of causal reasoning likely goes beyond composition and should include an explicit treatment of interventions and the ability to reason about hypothetical or unobserved scenarios .", "start_offset": 6035}, {"end_offset": 54075, "text": "Learning object-centric representations of multi-object\nscenes from multiple views.", "start_offset": 53992}, {"end_offset": 54437, "text": "Neural Information Processing Systems Workshop on Learning Disentangled Representations: from Perception\nto Control, 2017.", "start_offset": 54305}], "summary": true}], "figures": [{"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-8.jpg", "caption": "Figure 10: Photo of two leaf-tailed geckos \u2014 \u201cyoung and old\u201d \u00a9 2015 by Paul Bertner."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-13.jpg", "caption": "Figure 11: For partial objects (A) or only background (B), the occluded regions can be inpainted reasonably well, while in the case of full object occlusion (C) that is usually impossible."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-4.jpg", "caption": "Figure 12: Human perception is multistable, which is often demonstrated using visual illusions as in (a), yet it is also often encountered in the real world, e.g. for different groupings of tiles (b). To steer segregation towards a useful decomposition it is important to incorporate contextual information, for example to decide between a decomposition based on chairs or based on stacks in (c)."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-9.jpg", "caption": "Figure 13: Left: An illustration of (spectral) clustering approaches, which treat image segmentation as a graph-partitioning problem. Right: Corresponding instance segments as obtained by Isola et al. (2014)."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-17.jpg", "caption": "Figure 14: Left: An illustration of neural approaches that learn to directly output an image segmentation. Right: Corresponding bounding boxes and instance segments as obtained by He et al. (2017)."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-2.jpg", "caption": "Figure 15: Left: An illustration of attention-based approaches, which sequentially attend to individual objects. Right: Corresponding attention windows as obtained by Eslami et al. (2016)."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-3.jpg", "caption": "Figure 16: Left: An illustration of generative approaches to segregation that model an image as a mixture of components. Right: A corresponding decomposition in terms of individual objects as obtained by Greff et al. (2019)."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-5.jpg", "caption": "Figure 17: Three different objects ( , \u2022, F) appear in different pairings on a scale (a) and (b). By evaluating their relationships (d), it can be inferred how the scale will tip in (c)."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-19.jpg", "caption": "Figure 18: Three different ways in which structure can be defined in terms of relations between objects: As a factor graph, a directed graph, or as nested role-filler bindings."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-14.jpg", "caption": "Figure 19: Examples of different structural forms (Kemp and Tenenbaum, 2008) that each can be used to define relations among objects and imply different patterns of generalization."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-12.jpg", "caption": "Figure 1: Various evidence for shortcomings of current neural networks. (a) CNN image classifiers are biased towards texture over shape (Geirhos et al., 2019) and (b) can be well approximated by bag-of-local-features models (Brendel and Bethge, 2019). Hence, scrambling the image in a way that preserves local (but not global) structures affects them less than humans. (c) Neural network based agents trained on Breakout, fail to generalize to slight variations of the game such as a shifted paddle or an added middle wall (Kansky et al., 2017). (d) Neural networks also struggle to learn visual relations such as whether two shapes are the same or different (Fleuret et al., 2011; Kim et al., 2018)."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-11.jpg", "caption": "Figure 21: Illustration of several Gestalt Laws of visual perception. Note how the different cues influence which elements are perceived as belonging together."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-1.jpg", "caption": "Figure 23: Left: Two examples of visual search tasks, an easy one where the target \u201cpops-out\u201d (top) and a hard one that requires serial search. Middle: Diagram of processing operations involved in the perception of objects according to FIT. Right: Two example tasks that have been used to demonstrate illusory conjunctions. Note, that the effect cannot be reproduced in print because it relies on showing the images very briefly."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-16.jpg", "caption": "Figure 24: In an early experiment, Sidman (1971) examined a boy that could match spoken words to pictures and to name pictures (gray), but was unable to read. After being taught to match spoken words to written words (blue), he was then also able to read the written words aloud (red), and to match them to pictures (green). In this case, the dotted arrows represent relations that were never explicitly taught, and which were derived based on reflexivity (red) and transitivity (green) of the underlying equivalence relation (Sidman et al., 1989). Later, it was found that such derived relationships play an important role in systematically altering human behavior in response to feedback from the environment."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-0.jpg", "caption": "Figure 2: The binding problem in artificial neural networks can be understood from the perspectives of segregation, representation, and composition. Each of these subproblems focuses on a different functional aspect of dynamically binding neurally processed information with the aim of facilitating more symbolic information processing."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-10.jpg", "caption": "Figure 3: Illustration of the superposition catastrophe: A distributed representation in terms of disentangled features like color and shape (a, b) leads to ambiguity when confronted with multiple objects (c): The representation in (c) could equally stand for a red apple and a green pear, or a green apple and a red pear. It leads to an indiscriminate bag of features because there is no association of features to objects. A simple form of this problem in neural networks was first pointed out in Rosenblatt (1961), and has been debated in the context of neuroscience since (Milner, 1974; von der Malsburg, 1981)."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-18.jpg", "caption": "Figure 4: A visual scene composed of various unfamiliar objects."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-6.jpg", "caption": "Figure 5: Left: Interpretable features learned on ImageNet as observed in Olah et al. (2017). Right: Learned word embeddings have been demonstrated to capture some of the semantic structure of text (Mikolov et al., 2013), although to a lesser extent than was initially reported (Nissim et al., 2019)."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-15.jpg", "caption": "Figure 6: Illustration of the four different types of slot-based representations."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-7.jpg", "caption": "Figure 7: Illustration of the two main augmentation based approaches to object representations. Left: Neural activity over time for a temporal code, where synchronization is emphasized using color. Right: Complex valued activations are represented by arrows and colored according to their direction."}, {"id": "arXiv_pdf_2012_v2_0026@2012.05208-Figure-20.jpg", "caption": "Figure 9: Correspondence of attractor states to visual interpretations for a tri-stable variant of the Necker cube. The vector field illustrates the (input-dependent) inference dynamics in feature space, with one attractor for each stable interpretation."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Image Generation", "Transfer Learning", "Common Sense", "Machine Translation", "Multi-Task Learning", "Q-Learning", "Game of Go", "Image Classification", "Scene Understanding", "Dimensionality Reduction", "Real-Time Object Detection", "Instance Segmentation", "Graph Clustering", "Multi-Object Tracking", "Knowledge Base Completion", "Decision Making", "Speech Recognition", "Medical Diagnosis", "Trajectory Prediction", "Contour Detection", "Visual Question Answering", "Motion Segmentation", "Question Answering", "Relational Reasoning", "Boundary Detection", "Conditional Image Generation", "Computer Vision"], "datasets": ["ImageNet"], "queryEntities": {}}, {"id": "23f1e829-285f-34ad-8723-df48c7e479f6", "title": "Complex Relation Extraction: Challenges and Opportunities", "score": 0.0, "url": "https://arxiv.org/abs/2012.04821", "pdfUrl": "https://arxiv.org/pdf/2012.04821.pdf", "bibTexUrl": null, "authors": ["Haiyun Jiang", "Qiaoben Bao", "Qiao Cheng", "Deqing Yang", "Li Wang", "Yanghua Xiao"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Relation extraction aims to identify the target relations of entities in texts. Relation extraction is very important for knowledge base construction and text understanding. Traditional binary relation extraction, including supervised, semi-supervised and distant supervised ones, has been extensively studied and significant results are achieved. In recent years, many complex relation extraction tasks, i.e., the variants of simple binary relation extraction, are proposed to meet the complex applications in practice. However, there is no literature to fully investigate and summarize these complex relation extraction works so far. In this paper, we first report the recent progress in traditional simple binary relation extraction. Then we summarize the existing complex relation extraction tasks and present the definition, recent progress, challenges and opportunities for each task.", "startOffset": 0, "endOffset": 890}], "summary": false}, {"title": "Introduction", "figure": null, "fragments": [{"end_offset": 383, "text": "Traditional RE tasks aim to identify the correct relation between two entities from texts.", "start_offset": 271}, {"end_offset": 1054, "text": "Traditional RE tasks mainly focus on the binary relation between two entities. We refer to these tasks as binary relation extraction and they usually take learningbased solutions. According to the problem settings, the traditional BiRE tasks are roughly divided into three categories: supervised BiRE, semi-supervised BiRE, distant supervised BiRE. Specifically, supervised BiRE focuses on the learning of a RE model from a set of high-quality labeled data.", "start_offset": 580}, {"end_offset": 2801, "text": "To solve this problem, the task of few-shot relation extraction was proposed, which focuses on building effective models with just a few samples.", "start_offset": 2656}, {"end_offset": 4523, "text": "N-ary relation extraction aims to extract relations among n entities in the context of one or more sentences.", "start_offset": 4414}, {"end_offset": 5305, "text": "Conditional relation extraction aims to extract relations with certain constrains, e.g, temporal or spatial conditions, which are very important for complex applications. For example, we know the fact is only valid during 2008-2017. If this fact is used for knowledge-based question answering today, it may have serious political implications.", "start_offset": 4927}], "summary": true}, {"title": "Binary Relation Extraction", "figure": 0, "fragments": [{"end_offset": 186, "text": "Simple binary relation extraction has been extensively studied for many years. In general, BiRE can be categorized into: supervised, semi-supervised, distant supervised paradigms.", "start_offset": 0}, {"end_offset": 648, "text": "Supervised BiRE focuses on the learning of a RE model based on a set of high-quality labeled samples. These samples are widely obtained by manual annotation or careful crowdsourcing. Each sample is formalized as , where t = is an entity pair. st is a sentence containing t and it expresses the labeled relation r. A supervised BiRE model accepts t, st as inputs and predicts the proper relation r for entity pair t as the output.", "start_offset": 200}, {"end_offset": 774, "text": "In recent years, deep learning has been extensively used in RE tasks and many novel neural models are proposed.", "start_offset": 663}, {"end_offset": 3124, "text": "For example, [Carlson et al, 2010] adds constrains to the training procedure by coupling many extractors for different categories and relations. With the exploration of teacher-student models in semisupervised learning, [Luo et al, 2019] introduces this architecture into semi-supervised BiRE where students learn a robust representation from unlabeled data and teachers guide students with labeled data.", "start_offset": 2718}], "summary": true}, {"title": "Complex Relation Extraction", "figure": null, "fragments": [{"end_offset": 319, "text": "There are only very recent works around Complex Relation Extraction . Different from conventional BiRE, CoRE tries to extract more complex relations that involve multiple entities or under certain constrains. In this section, we present the definition and investigate the recent progress for each complex RE task.", "start_offset": 0}, {"end_offset": 573, "text": "In most cases, a relation only has fewer instances, which makes the traditional supervised RE models powerless. As a new paradigm, few-shot learning tends to be effective for this problem, i.e, few-shot RE.", "start_offset": 366}, {"end_offset": 1104, "text": "Few-shot RE aims to learn a functionF and predicts the proper relation y for the unlabeled sample x.\n proposes a new few-shot RE dataset: FewRel.", "start_offset": 931}, {"end_offset": 2711, "text": "In this task, the relation mentions can span multiple sentences and even paragraphs.", "start_offset": 2627}, {"end_offset": 4562, "text": "The cross-lingual RE also takes the sentence as well as entity mentions as input and outputs the relation between the given entity pair.", "start_offset": 4426}, {"end_offset": 7604, "text": "N-ary relation extraction aims to extract relations among n entities in the context of one or more sentences.", "start_offset": 7489}, {"end_offset": 15566, "text": "CopyR adopts an end-to-end neural model with copy mechanism to extract overlapping relations, which is the first work considering overlap problems.", "start_offset": 15399}], "summary": true}, {"title": "Conclusion", "figure": null, "fragments": [{"end_offset": 668, "text": "Relation extraction denotes a series of tasks that aim to identify the proper relations for two or more entities under specific settings. In this paper, we summarize the latest progress of simple binary RE tasks, including supervised, semi-supervised and distant supervised RE. Furthermore, we also investigate the more complex RE tasks, including the definition, recent progress, challenges and opportunities. Relationship extraction research is an eternal proposition, which mainly benefits from the advances in natural language processing and machine learning. We argue that the existing progress made in simple BiRE can also be transferred to the complex RE tasks.", "start_offset": 0}, {"end_offset": 845, "text": "We hope this survey make researchers quickly understand the concepts and research progress of each subtask in complex RE.", "start_offset": 724}], "summary": true}], "figures": [{"id": "arXiv_pdf_2012_v2_0024@2012.04821-Table-1.jpg", "caption": "Table 1: The SOAT results on SemEval-2010 Task 8. All the results are from [Zhao et al., 2019]."}, {"id": "arXiv_pdf_2012_v2_0024@2012.04821-Table-0.jpg", "caption": "Table 2: The SOAT results on NYT, where all the results are from [Li et al., 2019]. AUC denotes the area under the precision-recall curve."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Named Entity Recognition", "Transfer Learning", "Common Sense", "Relation Extraction", "Multi-Task Learning", "Visual Question Answering", "Question Answering", "Dependency Parsing", "Text Classification", "Reading Comprehension", "Knowledge Base Completion", "Relationship Extraction", "Document Classification"], "datasets": ["VQA", "SemEval", "DBpedia"], "queryEntities": {}}, {"id": "556a9b0a-753c-3d75-bd8c-bceb5dbe591d", "title": "PPKE: Knowledge Representation Learning by Path-based Pre-training", "score": 0.0, "url": "https://arxiv.org/abs/2012.03573", "pdfUrl": "https://arxiv.org/pdf/2012.03573.pdf", "bibTexUrl": null, "authors": ["Bin He", "Di Zhou", "Jing Xie", "Jinghui Xiao", "Xin Jiang", "Qun Liu"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Entities may have complex interactions in a knowledge graph (KG), such as multi-step relationships, which can be viewed as graph contextual information of the entities. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, and neglect most of the graph contextual information exists in the topological structure of KGs. In this study, we propose a Path-based Pre-training model to learn Knowledge Embeddings, called PPKE, which aims to integrate more graph contextual information between entities into the KRL model. Experiments demonstrate that our model achieves state-of-the-art results on several benchmark datasets for link prediction and relation prediction tasks, indicating that our model provides a feasible way to take advantage of graph contextual information in KGs.", "startOffset": 0, "endOffset": 834}], "summary": false}, {"title": "Introduction", "figure": 0, "fragments": [{"end_offset": 761, "text": "Knowledge graphs , such as WordNet , Freebase and Wikidata , aggregate a large amount of human knowledge and express in a structured way. The large number of triples in these KGs have constructed a complex knowledge network, but it is far from complete. In recent years, knowledge graph completion tasks have attracted great attention. Despite new state-of-the-art models emerge constently, most methods ignore the topological structure information of the KGs. Relation paths are the most common topological structure in KGs, and Figure 1 shows some relation path instances.", "start_offset": 0}, {"end_offset": 1067, "text": "Similar to word context in language models , relation paths can be considered as one kind of contextual information in KGs.", "start_offset": 924}, {"end_offset": 1899, "text": "Unreliable relation paths are common in knowledge graphs, and Lin et al. found that it is necessary to select reliable relation paths for knowledge representation learning.", "start_offset": 1720}, {"end_offset": 2201, "text": "Instead of relying on inference patterns, we propose PPKE, a path-based pre-training approach that integrates graph contextual information contained in relation paths into the model parameters.", "start_offset": 2008}, {"end_offset": 2583, "text": "During the path-based pre-training procedure, two-step relation paths are extracted from the knowledge graph and fed into the pre-training module with original triples. Then, the pre-trained model can be finetuned for downstream KGC tasks, such as link prediction and relation prediction.", "start_offset": 2295}], "summary": true}, {"title": "Related Work", "figure": null, "fragments": [{"end_offset": 1141, "text": "In the literature, relation paths have been utilized to improve the performance of KG-related tasks. One of the representative models is PtransE , which learns inference patterns from relation paths to improve the knowledge base completion tasks. In this work, a path-constraint resource allocation algorithm is proposed to measure the weights of inference patterns, and semantic composition of relation embeddings is utilized to represent relation paths. Despite its success, the modeling objects are limited to the inference patterns between relations and paths, and it did not model the contextual information that implicited in paths. Recently, several KRL methods have attempted to introduce more contextual information into knowledge representations. Relational Graph Convolutional Networks is proposed to learn entity embeddings from their incoming neighbors, which greatly enhances the information interaction between related triples. Nathani et al. further extend the information flow from 1-hop in-entities to n-hop during the learning process of entity representations.", "start_offset": 0}, {"end_offset": 1760, "text": "In this study, we develop a knowledge graph pre-training model to integrate more graph contextual information, and utilize this model to benefit KG-related tasks through a finetuning procedure.", "start_offset": 1567}], "summary": true}, {"title": "Methodology", "figure": 1, "fragments": [{"end_offset": 194, "text": "In this section, we will introduce our path-based pre-training model and how to finetune this model for downstream KGC tasks. Figure 2 presents the model architecture of path-based pre-training.", "start_offset": 0}, {"end_offset": 706, "text": "To distinguish the role of different entities and relations in the input sequence, position embeddings should be assigned to each input element. Besides, we move entities to the front of the input sequence to avoid the position bias in different sample lengths.", "start_offset": 445}, {"end_offset": 1946, "text": "Inspired by this task, we choose entities as the masked objects. In our framework, only one entity is masked in each input sample because the input length is relatively short. Take the relation path as an example, we formalize this sample into , then mask one of the entities and let the model make predictions, i.e,\nInput= \nLabel= \nwhere denotes an input element. As shown in Figure 2, we use a Transformer encoder to learn the graph contextual information of the inputs. Assuming e is the masked entity, and T is the output hidden state of input , the prediction objective is to calculate the prediction probability of entity e and maximize it.", "start_offset": 1090}, {"end_offset": 2667, "text": "Aligning\nwith the pre-trained model, the task objective is to predict the masked head entity.", "start_offset": 2568}], "summary": true}, {"title": "Experiments", "figure": 3, "fragments": [{"end_offset": 205, "text": "We conduct link prediction experiments on two widelyused benchmark datasets: FB15k-237 , which is a subset of Freebase; WN18RR , which is a subset of WordNet.", "start_offset": 0}, {"end_offset": 552, "text": "The detailed statistics of these datasets are listed in Table 1. During the model pre-training, only two-step relation paths are extracted from KGs. We count the number of quadruples in different training sets and the statistics is shown in Table 1.", "start_offset": 287}, {"end_offset": 880, "text": "In the pre-training phase, the maximum sequence length is set to 4. We use a batch size of 512 for WN18RR, and 4096 for FB15k-237 and FB15k.", "start_offset": 740}, {"end_offset": 1075, "text": "In this study, the Transformer layer number, head number and hidden size are set to 6, 4, 256, which are the same as that used in (Wang et al. 2019).", "start_offset": 926}, {"end_offset": 1338, "text": "We list six previous state-of-the-art models as baselines to compare the link prediction performance with our model, and the results demonstrate that PPKE outperforms on most evaluation metrics.", "start_offset": 1144}, {"end_offset": 1930, "text": "As we can see, KG-BERT and CoKE outperform PTransE and ProjE on FB15k, and PPKE achieves better results on both FB15k and FB15k-237. These experimental results verify that our model has the ability to utilize graph contextual information to improve the performance of link prediction and relation prediction.", "start_offset": 1622}, {"end_offset": 2566, "text": "In each sample group, triples and quadruples share the same head and tail entity.", "start_offset": 2485}], "summary": true}, {"title": "Conclusion and Future Work", "figure": null, "fragments": [{"end_offset": 559, "text": "We propose a novel approach to integrate graph contextual information into a path-based pre-training model, focusing on modeling one-step and two-step relations between entities. Experiments show our model outperforms previous state-of-the-art methods, which validates the intuition that graph contextual information is beneficial to knowledge graph completion tasks. In the follow-up work, we will try to add relation prediction objective into the pre-training procedure, and larger quantity or wider variety of graph contextual information will be explored.", "start_offset": 0}], "summary": true}], "figures": [{"id": "arXiv_pdf_2012_v2_0019@2012.03573-Figure-2.jpg", "caption": "Figure 1: An illustration of knowledge triples and relation paths. Circles are entities and arrows are relations."}, {"id": "arXiv_pdf_2012_v2_0019@2012.03573-Figure-0.jpg", "caption": "Figure 2: The model architecture of path-based pre-training. h, t denote the head and tail entity, r1, . . . , rn is the n-step relationship between the entity pair."}, {"id": "arXiv_pdf_2012_v2_0019@2012.03573-Figure-1.jpg", "caption": "Figure 3: Visualization for masked tail entities. Different colors mean different sample groups. Samples of the same color share the same head and tail entity. Round point represents hidden state representation for the masked tail entity in a triple while inverted triangle is for that in a quadruple."}, {"id": "arXiv_pdf_2012_v2_0019@2012.03573-Table-0.jpg", "caption": "Table 1: Statistics of the datasets. \u201cQuad\u201d means quadruple, i.e., 2-step relation path between two entities, such as (h, r1, r2, t). Datasets can be downloaded from this repository1."}, {"id": "arXiv_pdf_2012_v2_0019@2012.03573-Table-1.jpg", "caption": "Table 2: Link prediction results on WN18RR and FB15k-237. The results of TransE are reported by OpenKE (Han et al. 2018). The original CoKE model was implemented using deep learning framwork PaddlePaddle, and we re-implement this model using TensorFlow. The scores of remaining baselines follow their original papers."}, {"id": "arXiv_pdf_2012_v2_0019@2012.03573-Table-3.jpg", "caption": "Table 3: Relation prediction results (Hits@1) on FB15k and FB15k-237. The baselines are taken from original papers."}, {"id": "arXiv_pdf_2012_v2_0019@2012.03573-Table-2.jpg", "caption": "Table 4: Ablation study on the link prediction task."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Knowledge Graph Completion", "Semantic Composition", "Knowledge Base Completion"], "datasets": [], "queryEntities": {}}, {"id": "c02ceeb3-fe63-3055-913d-6291cc7016de", "title": "Exploring Neural Entity Representations for Semantic Information", "score": 0.0, "url": "https://arxiv.org/abs/2011.08951", "pdfUrl": "https://arxiv.org/pdf/2011.08951.pdf", "bibTexUrl": null, "authors": ["Andrew Runge", "Eduard Hovy"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Neural methods for embedding entities are typically extrinsically evaluated on downstream tasks and, more recently, intrinsically using probing tasks. Downstream task-based comparisons are often difficult to interpret due to differences in task structure, while probing task evaluations often look at only a few attributes and models. We address both of these issues by evaluating a diverse set of eight neural entity embedding methods on a set of simple probing tasks, demonstrating which methods are able to remember words used to describe entities, learn type, relationship and factual information, and identify how frequently an entity is mentioned. We also compare these methods in a unified framework on two entity linking tasks and discuss how they generalize to different model architectures and datasets.", "startOffset": 0, "endOffset": 813}], "summary": false}, {"title": "Introduction", "figure": null, "fragments": [{"end_offset": 393, "text": "Neural methods for generating entity embeddings have become the dominant approach to representing entities, with embeddings learned through methods such as pretraining, task-based training, and encoding knowledge graphs . These embeddings can be compared extrinsically by performance on a downstream task, such as entity linking .", "start_offset": 0}, {"end_offset": 1919, "text": "We address these gaps by comparing a wide range of entity embedding methods for semantic information using both probing tasks as well as downstream task performance. We propose a set of probing tasks derived simply from Wikipedia and DBPedia, which can be applied to any method that produces a single embedding per entity. We use these to compare eight entity embedding methods based on a diverse set of model architectures, learning objectives, and knowledge sources. We evaluate how these differences are reflected in performance on predicting information like entity types, relationships, and context words. We find that type information is extremely well encoded by most methods and that this can lead to inflated performance on other probing tasks.", "start_offset": 1166}, {"end_offset": 2451, "text": "We aim to provide a clear comparison of the strengths and weaknesses of various entity embedding methods and the information they encode to guide future work.", "start_offset": 2293}], "summary": true}, {"title": "Models", "figure": null, "fragments": [{"end_offset": 830, "text": "We compare eight different approaches to generating entity embeddings, organized along two dimensions: the training process of the underlying model, and the content used to inform the embeddings. Along the training dimension, the first method \nar X\niv :2\n0 1 1 .0. 8 9 5 1 v 1 \n1 7 N\no v 2\n0 2 0\nis task-learned embeddings, which are learned as part of a downstream task, such as EL. Pretrained embeddings are learned through a dedicated pretraining phase designed to produce entity embeddings. Finally, derived embeddings are produced by models capable of embedding any generic text, but that had no specific entity-based training. Along the content dimension, the first type is description-based embeddings, which are learned or generated from a text description of the entity.", "start_offset": 0}, {"end_offset": 1035, "text": "Lastly, graph-based embeddings are learned entirely from a knowledge graph, linking entities to types and to each other.", "start_offset": 915}, {"end_offset": 1495, "text": "We use the March 5, 2016 dump of Wikipedia to train our task-learned and pretrained embedding models, while the derived embedding models are publicly available pre-trained language models.234. For our task-learned models, we re-implement two neural EL models, which learn entity representations for the goal of connecting mentions of entities in text to entities in a knowledge base .", "start_offset": 1107}, {"end_offset": 1707, "text": "First is the CNN-based model of Francis-Landau et al, a description and context-based hybrid model.", "start_offset": 1600}], "summary": true}, {"title": "Entity Embedding Probing Tasks", "figure": null, "fragments": [{"end_offset": 953, "text": "By ensuring each word appears both in context with an entity and in the description, we can avoid biasing the task towards context or\ndescription-based embeddings.", "start_offset": 790}, {"end_offset": 1381, "text": "Similar to prior work , we examine how well different entity embedding methods are able to learn entity type information using probing tasks based on the DBPedia6 ontology.", "start_offset": 1157}, {"end_offset": 3734, "text": "We use the 244 extracted relationship types from the relation identification task to create a 244-way relationship classification task .", "start_offset": 3593}, {"end_offset": 4095, "text": "If the representations can detect entity types effectively, then certain types of relationships may be easier to classify based solely on the types of the entities involved.", "start_offset": 3922}, {"end_offset": 4741, "text": "Finally, we examine the general task of predicting whether a pair of entities is related or not, which requires an explicit relationship between two entities compared to an entity relatedness task . Effectively encoding this information can help with tasks like knowledge graph completion, where knowing the existence of a link is useful, even if the exact type of the link is unknown.", "start_offset": 4305}, {"end_offset": 6293, "text": "Finally, we explore a small set of factual knowledge probes for spatial, temporal, and numeric information using triples of literals from DBPedia. The first two tasks probe if the embeddings retain the century or decade that a given person was born, based on the embedding for that person .", "start_offset": 5977}, {"end_offset": 7416, "text": "We use relatively small training sizes and a logistic regression classifier as the probing model to observe how easily the information can be identified from a limited sample and simple model .", "start_offset": 7199}, {"end_offset": 7989, "text": "We report macro F1 for all tasks except\nbinary relation detection and context word prediction, where we report macro F1 averaged over all sub-tasks, and the popularity regression task where we report RMSE.", "start_offset": 7784}], "summary": true}, {"title": "Probing Experiment Analysis", "figure": 1, "fragments": [{"end_offset": 105, "text": "We present the results of our probing tasks in Tables 1 and 2 and analyze them in the following sections.", "start_offset": 0}, {"end_offset": 622, "text": "Ganea performs best on the two word context tasks, beating even BERT and BERT-large, and demonstrating one of the advantages of their shared word and entity embedding space. CNN performs on par with the BOW model, indicating that the task-learned embeddings store a fair amount of lexical information to complete the EL task. The RNN embeddings perform at almost chance level, which could mean the lexical information is stored in the RNN layers and not transferred to the entity embeddings.", "start_offset": 131}, {"end_offset": 3569, "text": "While strong performance from BigGraph and Wiki2V may be expected, high scores by models like BERT or CNN, which had no explicit training on relationships and performed poorly on relation detection, prompt further examination. Some of the best performing tasks for these models feature a less common entity as the head and a more frequent entity as the tail, such as biological classifications and location-related relationships .", "start_offset": 3083}, {"end_offset": 5367, "text": "Identifying these relationships based on entity type would be easy, but introducing negative examples with matching fine-grained entity types will harm performance much more for models that primarily rely on type information.", "start_offset": 5142}, {"end_offset": 7374, "text": "Table 1 shows that Wiki2V performs best on both the popularity regression task and the binned popularity task, particularly outperforming the other models on the regression task.", "start_offset": 7196}, {"end_offset": 8661, "text": "Ganea overtakes Wiki2V on all comparative tasks, while BERT and BERT-Large only surpass it on the coarser P-5 and P-10 tasks.", "start_offset": 8536}, {"end_offset": 10680, "text": "BERT and BERTLarge have access to the description which often contains a birth year that they can encode directly, yielding higher performance on birth decade prediction than any other model.", "start_offset": 10489}], "summary": true}, {"title": "Downstream Task - Entity Linking", "figure": 3, "fragments": [{"end_offset": 136, "text": "Many of our embedding methods have been evaluated on EL tasks in prior work, either in a separate model or as full EL models themselves.", "start_offset": 0}, {"end_offset": 1005, "text": "We test the embeddings using three EL models on two standard EL datasets, the AIDA-CoNLL 2003 dataset and the TACKBP 2010 dataset . Two of our EL models are the CNN and RNN EL models used to generate our task-learned embeddings.", "start_offset": 737}, {"end_offset": 1588, "text": "To compare the impact of the entity embeddings, we replace the candidate document convolution in the CNN model or the randomly initialized embeddings in the RNN and transformer models with the pretrained embeddings during training.", "start_offset": 1357}, {"end_offset": 2763, "text": "While the CNN and RNN embeddings provide improvements compared to using no pretrained embeddings, they transfer poorly to other models, often performing worse than even the simple BOW embedding approach. BigGraph performs worse than BOW on AIDA-CoNLL, but better on TAC-KBP, which could indicate its strong type information helps more on the smaller dataset, while word information may be more helpful on AIDA. While the transformer EL model clearly outperforms the CNN and RNN EL models, no single embedding model performs consistently better across these datasets and models.", "start_offset": 2186}, {"end_offset": 3251, "text": "Ganea performs extremely well in combination with the Transformer model, approaching the current state of the art on AIDA-CoNLL set by Raiman and Raiman .", "start_offset": 3091}, {"end_offset": 3658, "text": "Tasks like knowledge base completion or question answering will require additional information and our probing task results may provide guidance for selecting embeddings for those tasks.", "start_offset": 3472}], "summary": true}, {"title": "Related Work", "figure": null, "fragments": [{"end_offset": 520, "text": "Interpretation of neural language representations has drawn increased attention in recent years, particularly with the rise of BERT and transformerbased language models . We focus on methods for detecting specific attributes in learned representations, referred to as point-based intrinsic evaluations , auxiliary prediction tasks or probing tasks .", "start_offset": 0}, {"end_offset": 1068, "text": "These techniques have similarly been applied to entity embeddings, though usually to limited extents. Entity type prediction has been among the most common task explored when proposing a new entity embedding method, in part because fine-grained entity type prediction is a common standalone task itself .", "start_offset": 652}, {"end_offset": 1788, "text": "Concurrent with this work, Chen et al. introduced EntEval, a series of probing tasks for both fixed and contextual entity embeddings to evaluate semantic type and relationship information in BERT and ELMo-based entity embeddings.", "start_offset": 1532}, {"end_offset": 2530, "text": "Our work builds on this prior work, which has often limited either its task exploration, the models being evaluated, or the extent of its analysis. We propose a set of tasks including several which are, to the best of our knowledge, novel to analyzing entity embeddings such as popularity prediction and context word evaluation. These tasks can be easily applied to any method which produces a single embedding per entity allowing us to compare a much wider range of model architectures than in any prior work.", "start_offset": 2020}, {"end_offset": 3236, "text": "Approaches based on skip-gram and CBOW models jointly trained word and entity embeddings, producing state of the art results on EL , named entity recognition , and question answering .", "start_offset": 2931}, {"end_offset": 3697, "text": "Recent approaches have explored integrating BERT with pretrained entity embeddings , while others have used BERT directly to learn entity embeddings for the task .", "start_offset": 3416}], "summary": true}, {"title": "Conclusion", "figure": null, "fragments": [{"end_offset": 158, "text": "In this work, we propose a new set of probing tasks for evaluating entity embeddings which can be applied to any method that creates one embedding per entity.", "start_offset": 0}, {"end_offset": 1080, "text": "Overall, we find that while BERT-based entity embeddings perform well on many of these tasks, their high performance can often be attributed to strong entity type information encoding. More specialized models such as Wikipedia2Vec are better able to detect and identify relationships, while the embeddings of Ganea and Hofmann better capture the lexical and distributional semantics of entities.", "start_offset": 678}, {"end_offset": 1708, "text": "Our work provides insight into the information encoded by static entity embeddings, but entities can change over time, sometimes quite significantly.", "start_offset": 1559}, {"end_offset": 2078, "text": "Context-based embeddings in particular could then be dynamically updated with new information, instead of being retrained from scratch.", "start_offset": 1943}], "summary": true}, {"title": "A Entity Linking Task and Model Configuration", "figure": null, "fragments": [{"end_offset": 709, "text": "A.1 Data Preprocessing\nAs described above, we use two standard entity linking datasets for evaluation, the AIDA-CoNLL 2003 dataset and the TACKBP 2010 dataset . Following prior work , we evaluate only the mentions that have valid entries in the KB. TAC-KBP does not have a dedicated validation set, so we assign a random 10% of the training data to the validation set. For candidate set generation for AIDA-CoNLL, we use the PPRforNED candidate sets . For TAC-KBP, we pick candidates for each mention that either match the mention, a word in the mention, or have an anchor text that matches the mention.", "start_offset": 0}, {"end_offset": 1167, "text": "For the CNN and RNN models, we lowercase both the KB and mention text,\nomit stop words and punctuation, and replace all numbers with a single token.", "start_offset": 1019}, {"end_offset": 1493, "text": "To insert pretrained entity embeddings, we replace the candidate document convolution layer, followed by a single layer MLP to reduce the embedding to 150 dimensions.", "start_offset": 1327}, {"end_offset": 3002, "text": "When using pretrained entity embeddings, we replace the randomly initialized entity embeddings with our pretrained embeddings and randomly initialize any missing entity embeddings from a normal distribution with mean 0 and standard deviation 0.02.", "start_offset": 2755}, {"end_offset": 3702, "text": "During training, we apply gradient clipping of 1.0 for the transformer model and 5.0 for the CNN and RNN.", "start_offset": 3597}, {"end_offset": 4380, "text": "When training the CNN and RNN models on the Wikipedia EL corpus, we use all the same model and training settings as described above, but use batch size 512.", "start_offset": 4224}], "summary": true}], "figures": [{"id": "arXiv_pdf_2011_v2_0044@2011.08951-Figure-0.jpg", "caption": "Figure 1: Person-Person relationship confusion matrix for Wiki2V"}, {"id": "arXiv_pdf_2011_v2_0044@2011.08951-Table-0.jpg", "caption": "Table 1: Results for context word, entity type, relationship and popularity probing tasks. All values are micro F1, except for W-H, W-M, and R-I, which report average macro F1 across all subtasks, and P-R which reports RMSE."}, {"id": "arXiv_pdf_2011_v2_0044@2011.08951-Table-2.jpg", "caption": "Table 2: Results for the factual probing tasks."}, {"id": "arXiv_pdf_2011_v2_0044@2011.08951-Table-1.jpg", "caption": "Table 3: Entity linking performance of 3 models both without pretrained embeddings and using each of our 8 entity embedding methods. Best values for each model and dataset are in bold."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Knowledge Graph Completion", "Named Entity Recognition", "Entity Linking", "Knowledge Base Completion", "Question Answering"], "datasets": ["DBpedia", "CoNLL 2003"], "queryEntities": {}}, {"id": "acf49eb7-fd00-3808-8f5f-6aa4de2f0153", "title": "Association Rules Enhanced Knowledge Graph Attention Network", "score": 0.0, "url": "https://arxiv.org/abs/2011.08431", "pdfUrl": "https://arxiv.org/pdf/2011.08431.pdf", "bibTexUrl": null, "authors": ["Zhenghao Zhang", "Jianbin Huang", "Qinglin Tan"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Most existing knowledge graphs suffer from incompleteness. Embedding knowledge graphs into continuous vector spaces has recently attracted increasing interest in knowledge base completion. However, in most existing embedding methods, only fact triplets are utilized, and logical rules have not been thoroughly studied for the knowledge base completion task. To overcome the problem, we propose an association rules enhanced knowledge graph attention network (AR-KGAT). The AR-KGAT captures both entity and relation features for high-order neighborhoods of any given entity in an end-to-end manner under the graph attention network framework. The major component of AR-KGAT is an encoder of an effective neighborhood aggregator, which addresses the problems by aggregating neighbors with both association-rules-based and graph-based attention weights....", "startOffset": 0, "endOffset": 853}], "summary": false}, {"title": "Introduction", "figure": 0, "fragments": [{"end_offset": 455, "text": "Over recent years, large-scale knowledge base, such as Freebase , DBpedia , and YAGO, have been developed to store structured information of common knowledge. KBs are represented as directed multirelational graphs, Knowledge Graphs , where entities and relations are represented as nodes and as edges of different types, respectively. They usually consist of numerous facts structured in the triplets: <head entity, relation, tail entity>, e.g.", "start_offset": 0}, {"end_offset": 920, "text": "Many large-scale KGs have been constructed with millions\nof entities and relations.", "start_offset": 837}, {"end_offset": 1718, "text": "To alleviate this drawback, relation prediction, also referred to as knowledge base completion, infers the missing facts based on the given facts. Enormous efforts have been devoted to knowledge base completion to estimate missing relations between entities under the supervision of the existing knowledge graph.", "start_offset": 1406}, {"end_offset": 4318, "text": "Thus, to transfer human knowledge to entity and relation embedding, the logical rules can be integrated into knowledge graph embedding, strengthening the learning process. In order to overcome the limitations, we propose an end-to-end GAT framework for multi-relational knowledge graphs, Association Rules Enhanced Knowledge Graph Attention Network .", "start_offset": 3959}, {"end_offset": 6331, "text": "The contributions of this paper are as follows: Triplets and logical rules are jointly modeled in the proposed unified framework to achieve more predictive entity and relation embeddings.", "start_offset": 6141}, {"end_offset": 6873, "text": "Weights of cascaded propagations are generated through the integration of logic- and graph-based attention mechanisms to reveal the importance of such high-order connectivThe Computer Journal, Vol.", "start_offset": 6676}], "summary": true}, {"title": "Related Work", "figure": null, "fragments": [{"end_offset": 247, "text": "Recently, several variants of knowledge graph embedding methods have been proposed for relation prediction. These methods can be broadly classified as: translational based, convolutional neural network based, and graph neural network based models.", "start_offset": 0}, {"end_offset": 736, "text": "Starting with TransE, there have been multiple proposed approaches that use simple operations like dot products and matrix multiplications to compute a score function. Other transition-based models extend TransE to additionally use projection vectors or matrices to translate head and tail embeddings into the relation vector space, such as: TransH , TransR , and TransD , STransE .", "start_offset": 339}, {"end_offset": 911, "text": "To obtain multiple representations of an entity, TransH projects an entity vector into relation-specific hyperplanes.", "start_offset": 794}, {"end_offset": 2456, "text": "In contrast, Convolution based models learn more expressive representations due to their parameter efficiency and consideration of complex relations.", "start_offset": 2307}, {"end_offset": 3628, "text": "In [21], the authors propose applying Graph Neural Networks on the KG, which generates the embedding of a new entity by aggregating all its known neighbors.", "start_offset": 3465}, {"end_offset": 4358, "text": "A graph based neural network model called R-GCN is an extension of applying graph convolutional networks to relational data.", "start_offset": 4217}, {"end_offset": 5380, "text": "Recent works have shown that the inclusion of background knowledge, such as logical rules, can improve the performance of embeddings in downstream machine learning tasks.", "start_offset": 5210}, {"end_offset": 5590, "text": "Wei et al. tried to leverage both embedding methods and logical rules into the knowledge graph embedding for KG completion.", "start_offset": 5462}], "summary": true}, {"title": "Preliminaries", "figure": null, "fragments": [{"end_offset": 402, "text": "Knowledge Graphs can be represented by a collection of valid factual triples in the form of (head entity, relation, tail entity) denoted as .", "start_offset": 246}, {"end_offset": 1017, "text": "We denote the projection of NK on E and R by NE and NR , respectively. Here NE are neighbors and NR are neighboring relations. Knowledge graph embedding is an effective way to parameterize entities and relations as vector representations, while preserving the graph structure.", "start_offset": 727}, {"end_offset": 1453, "text": "Finally, to learn the entity and relation representations, an optimization problem is solved for maximizing the plausibility of the triple in the KG.", "start_offset": 1304}, {"end_offset": 2371, "text": "The training of TransE aims to optimize the discrimination between positive triplets and negative ones.", "start_offset": 2268}, {"end_offset": 2596, "text": "To address the shortcomings of GCNs, introduced Graph Attention Networks .", "start_offset": 2516}, {"end_offset": 3529, "text": "Here, the relative attention value can be computed using a softmax function over all the values in the neighborhood.", "start_offset": 3413}], "summary": true}, {"title": "Association Rules Mining", "figure": 1, "fragments": [{"end_offset": 238, "text": "In the proposed framework, the input and output are triplets of the knowledge graph G and the association rules of the two types with corresponding scores. The main steps of the proposed framework are described in this section as follows.", "start_offset": 0}, {"end_offset": 3261, "text": "This transitivity rule above denotes that if x and y are linked by relation s1 and y and z are linked by s2, x and z will be linked by relation t.", "start_offset": 3115}, {"end_offset": 4045, "text": "In this paper, both one-toone and n-to-one association rules are used to discover correlations in Knowledge Base .", "start_offset": 3927}, {"end_offset": 4906, "text": "Samples of the rule are extracted from given triplets in this step.", "start_offset": 4839}, {"end_offset": 6209, "text": "Here, fa and fb are defined as two constituent formulas, either atomic or complex, which consist of a single atom or a set of atoms with logical connectives.", "start_offset": 6042}, {"end_offset": 7337, "text": "As an empirical statistic over the entire KG, psupport is larger if more entities with neighboring formula fa also have fb as an association rule. For example, there are 10000 entities in a KB Dataset, including 6000 entities with neighbor formula f1, 7500 with f2, and 4000 with both.", "start_offset": 7042}, {"end_offset": 8624, "text": "Definition 5 : It represents the ratio of confidence degree pConfidence to p , and can be shown as follows:\nppromotion = pConfidence \np \nThe promotion degree reflects the correlation between fa and fb in association rule .", "start_offset": 8337}, {"end_offset": 9496, "text": "As shown in Figure x, the input of this framework is triples of the knowledge graph, and the output is the ground rules of different types with corresponding promotion degrees.", "start_offset": 9320}], "summary": true}, {"title": "AR-KGAT: Association Rules Enhanced Knowledge Graph Attention Network", "figure": 3, "fragments": [{"end_offset": 438, "text": "In the attention-based embedding propagation layer, the first-order connectivity information is explicitly leveraged to relate representations of entity and relation. Although the performance of GATs was proved, they cannot be used for KGs because relation information and logical rules in the neighborhood are neglected. As an integral part of KGs. they could promote more effective aggregation of the transformed representations.", "start_offset": 0}, {"end_offset": 987, "text": "With entity embeddings learned from the encoder module as input, the decoder aims to measure the plausibility of triplets and logical rules.", "start_offset": 847}, {"end_offset": 3329, "text": "Considering these two requirements, a novel association rules-enhanced attention-based aggregator is proposed, incorporating both logic-based and graph-based attention mechanisms.", "start_offset": 3150}, {"end_offset": 3758, "text": "Traditional methods for aggregating neighborhoods ignored useful information in the neighbor node ej and high-order relation rk since only the collections of transformed embeddings were considered into account.", "start_offset": 3548}, {"end_offset": 6763, "text": "To address this issue, we adopt an attention-based mechanism shown in Figure 4:\nSimilar to GAT, the proposed attention mechanism is applied to learn the importance of each triplet b automatically.", "start_offset": 6560}, {"end_offset": 8722, "text": "Representations of all the relations in the logical formula are summarized to compute the auxiliary relation.", "start_offset": 8613}, {"end_offset": 13134, "text": "In our proposed AR-KGAT framework, any rules represented as the first-order logic formula can be handled besides these two types of rules.", "start_offset": 12996}, {"end_offset": 13363, "text": "The dataset is used to train the proposed model, where the global loss is minimized to learn the entity and relation embedding representation.", "start_offset": 13221}], "summary": true}, {"title": "Experimental Evaluation", "figure": 10, "fragments": [{"end_offset": 807, "text": "The performance of the proposed model AR-KGAT, especially the embedding propagation layer, is evaluated on three real-world datasets in typical tasks of knowledge graph completion: the link prediction and triplet classification. In this section, first, the performance of the proposed AR-KGAT is compared with state-ofthe-art knowledge graph embedding methods. Second, the contribution analysis of each component of the proposed AR-KGAT, including association rules, attention mechanism, and aggregator selection, are also analyzed. Lastly, the effects of the hyper-parameters of the proposed AR-KGAT are analyzed. In the experiments, the performance of the proposed model is evaluated on two widely used public datasets, WN18RR and FB15k-237 , for link prediction and triplet classification tasks.", "start_offset": 0}, {"end_offset": 6227, "text": "In the experiments, the effectiveness of the proposed model is evaluated with state-of-the-art methods on two typical tasks for knowledge graph completion: link prediction and triplet classification .", "start_offset": 6000}, {"end_offset": 9335, "text": "The logical rule-enhanced models, including LR-KGE and AR-KGAT, mostly outperform the original models, where triplets are used alone on both datasets.", "start_offset": 9185}, {"end_offset": 12339, "text": "For these results, we conclude that the translational characteristic between relation and entity can be effectively kept in CNN or GNN based models using the neural network, achieving promising performance in the triplet classification.", "start_offset": 12103}, {"end_offset": 13391, "text": "The importance of the attention mechanism is proved by the consistently superior performance of the AR-KGAT over the ARKGCN, showing it is one of the key components of the proposed model.", "start_offset": 13204}, {"end_offset": 14643, "text": "As a basic graph attention network framework, AR-KGAT-TriOnly uses only triplets in the optimization function and aggregator to conduct the embedding task.", "start_offset": 14488}], "summary": true}, {"title": "Conclusion And Future Work", "figure": null, "fragments": [{"end_offset": 591, "text": "This paper proposes a joint embedding framework, ARKGAT, for knowledge graphs and logical rules. The key idea is to integrate triplets and association rules in the knowledge graph attention network framework to generate effective representations. Specifically, the graph attention mechanisms are generalized and extended so that both entity and relation features are captured in a multi-hop neighborhood of a given entity. In our proposed aggregator, the weights of a coarse relation level and a fine neighbor level are estimated by logical rules and neural attention networks, respectively.", "start_offset": 0}, {"end_offset": 856, "text": "In this way, the learned embeddings are certainly more useful for knowledge acquisition and inference, which are compatible with triplets and rules.", "start_offset": 708}, {"end_offset": 1000, "text": "Each dataset is used for both the triplet classification and link\nThe Computer Journal, Vol.", "start_offset": 908}, {"end_offset": 1225, "text": "Experimental results indicate that significant and consistent performance improvement is achieved through the joint embedding over state-ofthe-art models on two typical KG completion tasks.", "start_offset": 1036}, {"end_offset": 1522, "text": "Future works will include extending the proposed method to hierarchical graphs, capturing higher-order relations between entities in our graph attention model.", "start_offset": 1363}, {"end_offset": 1718, "text": "The embedding of multiple-source knowledge graphs will also be explored, e.g, jointly embedding Freebase and YAGO.", "start_offset": 1603}, {"end_offset": 1992, "text": "Any opinions, findings and conclusions expressed here are those of the authors and do not necessarily reflect the views of the funding agencies.", "start_offset": 1848}], "summary": true}], "figures": [{"id": "arXiv_pdf_2011_v2_0041@2011.08431-Figure-0.jpg", "caption": "FIGURE 1: Example of a knowledge graph. Subgraph consists of the relationships between entities (solid lines) and inferred relationships (dashed lines). Inferred relationships are initially hidden."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Figure-5.jpg", "caption": "FIGURE 2: The overall framework of association rules mining from a knowledge graph."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Figure-2.jpg", "caption": "FIGURE 3: Illustration of the proposed AR-KGAT model."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Figure-1.jpg", "caption": "FIGURE 4: The attentive embedding propagation layer of the proposed model."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Figure-6.jpg", "caption": "FIGURE 7: Number of neighbor relations study using FB15k-237 and WN18RR datasets."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Figure-3.jpg", "caption": "FIGURE 8: Effect of the varying embedding dimension (d)."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Figure-4.jpg", "caption": "FIGURE 9: Effect of the proportion of unseen entities on FB15k-237 dataset."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-5.jpg", "caption": "TABLE 10: Effect of association rules on FB15k-237 and WN18RR for link prediction."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-6.jpg", "caption": "TABLE 1: Statistics of the datasets."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-8.jpg", "caption": "TABLE 2: Examples of rules created"}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-9.jpg", "caption": "TABLE 3: The rule statistics."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-7.jpg", "caption": "TABLE 4: The candidates of ground rule."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-4.jpg", "caption": "TABLE 5: Results of link prediction on the testing dataset of FB15k-237 and WN18RR."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-3.jpg", "caption": "TABLE 6: The accuracy comparison of triplet classification for the FB15k-237 and WN18RR test sets."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-0.jpg", "caption": "TABLE 7: Link prediction results by relation category for LR-KGE and AR-KGAT on FB15k-237. The complex relations are more effectively captured in the proposed AR-KGAT model than the rule-enhanced LR-KGE model."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-1.jpg", "caption": "TABLE 8: Effect of association rules on FB15k-237 and WN18RR for link prediction."}, {"id": "arXiv_pdf_2011_v2_0041@2011.08431-Table-2.jpg", "caption": "TABLE 9: Effect of association rules on FB15k-237 and WN18RR for triple classification."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Knowledge Graph Completion", "Knowledge Graph Embedding", "Node Classification", "Relation Extraction", "Knowledge Base Completion", "Question Answering"], "datasets": ["DBpedia"], "queryEntities": {}}, {"id": "aa116576-5401-3eb6-b00e-99085304eaa0", "title": "Node Attribute Completion in Knowledge Graphs with Multi-Relational Propagation", "score": 0.0, "url": "https://arxiv.org/abs/2011.05301", "pdfUrl": "https://arxiv.org/pdf/2011.05301.pdf", "bibTexUrl": null, "authors": ["Eda Bayram", "Alberto Garcia-Duran", "Robert West"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "The existing literature on knowledge graph completion mostly focuses on the link prediction task. However, knowledge graphs have an additional incompleteness problem: their nodes possess numerical attributes, whose values are often missing. Our approach, denoted as MrAP, imputes the values of missing attributes by propagating information across the multi-relational structure of a knowledge graph. It employs regression functions for predicting one node attribute from another depending on the relationship between the nodes and the type of the attributes. The propagation mechanism operates iteratively in a message passing scheme that collects the predictions at every iteration and updates the value of the node attributes. Experiments over two benchmark datasets show the effectiveness of our approach.", "startOffset": 0, "endOffset": 808}], "summary": false}, {"title": "Introduction", "figure": 0, "fragments": [{"end_offset": 609, "text": "Knowledge graphs consist of structured data formed by semantic entities connected through multiple types of relationships. They play an important role in a wide variety of AI applications including question answering , drug discovery , and ecommerce . In the last years, this has led to immense attention on knowledge graph completion methods, which aim at inferring missing facts in a KG by reasoning about the observed facts . Knowledge graph embedding methods are at the core of this progress, by learning latent representations for both entities and relations in a KG .", "start_offset": 0}, {"end_offset": 1281, "text": "Despite the very large number of KGE and GNN methods, these works have mostly addressed link prediction and node/graph classification problems, respectively. While KGE methods always harness features learned from the relational structure of the graph, they very often overlook other information contained in the KGs such as the numerical properties of the entities.", "start_offset": 916}, {"end_offset": 2167, "text": "Different to the standard KG completion problems, in node attribute completion task, we harness not only the relational structure of the graph, but also the correlation between various types of node attributes.", "start_offset": 1957}, {"end_offset": 5057, "text": "The authors in [12] exploit such numerical node attributes in a KG for the multi-relational link prediction task.", "start_offset": 4944}, {"end_offset": 6144, "text": "The main difference of the proposed method from those is that it propagates incomplete node features across the graph instead of propagating fixed dimension of node representation vectors.", "start_offset": 5956}, {"end_offset": 6641, "text": "Similar to an attention mechanism, in our method, the assigned weights for the regression functions capture the importance of the collected predictions for a certain attribute in a node.", "start_offset": 6455}], "summary": true}, {"title": "Multi-Relational Attribute Propagation Algorithm", "figure": 1, "fragments": [{"end_offset": 808, "text": "MRAP explicitly makes use of the multi-relational structure given by the KG and the observed numerical node attributes to infer the missing ones.", "start_offset": 663}, {"end_offset": 1038, "text": "For instance, in Figure 1, one may provide an estimate about the date of death of Francis Ford Coppola by looking at the release date of one of his most popular movies.", "start_offset": 870}, {"end_offset": 2137, "text": "The proposed method, MRAP, recovers the values of missing node attributes by minimizing their distances to the predictions collected from such internal and external sources of information based on their weights.", "start_offset": 1926}, {"end_offset": 5151, "text": "While MRAP imputes the missing node attributes by iteratively applying Eq. and (3), the regression functions and their associated weights are computed in advanced, and kept fixed during the propagation process.", "start_offset": 4937}, {"end_offset": 5903, "text": "For instance, the attribute date of birth of a node can be estimated through a certain value difference from that of a neighbor connected via the relation type has child.", "start_offset": 5733}, {"end_offset": 6601, "text": "Note that the multi-relational GNN works mentioned in Section 1 usually apply a relation specific transformation to the embedding of a node to regress a feature of a neighboring node.", "start_offset": 6418}, {"end_offset": 6806, "text": "In our case, however, we do not have a fixed dimension of node feature vector, where the number of attributes assigned to each node varies.", "start_offset": 6667}, {"end_offset": 7672, "text": "Thus, the parameters of the regression functions are estimated from the observed set of node attributes.", "start_offset": 7568}, {"end_offset": 9078, "text": "Let V denote the set of nodes for which both the attributes y and x are observed as yv and xv respectively.", "start_offset": 8966}, {"end_offset": 10153, "text": "In our approach, functions AGGREGATE and COMBINE correspond to Eq. and (3), respectively.", "start_offset": 10060}], "summary": true}, {"title": "Experiments", "figure": 7, "fragments": [{"end_offset": 141, "text": "We evaluate the performance of the proposed method on two KG datasets whose nodes have numerical attributes: FB15K-237 and YAGO15K .", "start_offset": 0}, {"end_offset": 637, "text": "The number of node attributes of each type encountered in each dataset are also listed in Table 1. Two error metrics are used to assess the performance: Mean Absolute Error and Root Mean Square Error , which are measured on each type of attribute individually.", "start_offset": 365}, {"end_offset": 2921, "text": "For each type of attribute, NAP++ constructs a k-NN graph upon the learned node embedding solely for the propagation of that type of attribute. As\nopposed to these methods, MRAP leverages the correlations across all attribute types and the multi-relational structure of the KG to impute the missing values.", "start_offset": 2615}, {"end_offset": 3915, "text": "The performances of the methods on the two KG datasets are given in Table 4 and 5.", "start_offset": 3833}, {"end_offset": 4594, "text": "We argue that this is achieved because MRAP profits the message passing between different types of attributes, unlike the other methods, which do not permit a direct information exchange between them. This is found to be critical particularly among the date attributes: when the message passing between different types of attributes is deactivated in MRAP, the prediction error for most of the date attributes raises.", "start_offset": 4177}, {"end_offset": 5544, "text": "We see that the inner-node message passing is significant in particular between the attributes date of birth and date of death, area and population, and then, height and weight.", "start_offset": 5367}, {"end_offset": 5846, "text": "In Table 4 and 5, LOCAL/GLOBAL reports the best performance obtained by either of the two baselines for each attribute and an asterisk indicates that GLOBAL outperforms LOCAL.", "start_offset": 5667}], "summary": true}, {"title": "Conclusion", "figure": null, "fragments": [{"end_offset": 574, "text": "We address a relatively unexplored problem, node attribute completion, in knowledge graphs, and present MRAP, a multi-relational propagation algorithm to predict the missing node attributes. MRAP is framed in a message passing scheme, enabling the propagation of information across multiple types of attributes and over multiple types of relations. We show that MRAP very often outperforms several baselines in two datasets. Future work will focus on simultaneously learning the parameters of the regression functions while propagating the attributes in the knowledge graph.", "start_offset": 0}], "summary": true}], "figures": [{"id": "arXiv_pdf_2011_v2_0025@2011.05301-Figure-0.jpg", "caption": "Fig. 1: A part of KG data with incomplete node attributes"}, {"id": "arXiv_pdf_2011_v2_0025@2011.05301-Figure-1.jpg", "caption": "Fig. 2: Message passing performed by MRAP to update the attribute date of death for the node Francis Ford Coppola."}, {"id": "arXiv_pdf_2011_v2_0025@2011.05301-Figure-3.jpg", "caption": "Fig. 3: A summary of FB15K-237 with entity types and numerical attributes encountered on them. The number attached to the connection between a pair of entity types indicates the number of relationship types between those entities."}, {"id": "arXiv_pdf_2011_v2_0025@2011.05301-Figure-2.jpg", "caption": "Fig. 4: Histograms and fitted normal curves of node attribute differences computed along some relations"}, {"id": "arXiv_pdf_2011_v2_0025@2011.05301-Table-2.jpg", "caption": "Table 1: Number of node attributes encountered in datasets for each attribute type. The upper block contains numerical attributes of date type. The lower block contains all other attributes. A dash (-) indicates the corresponding attribute is not encountered in the dataset."}, {"id": "arXiv_pdf_2011_v2_0025@2011.05301-Table-3.jpg", "caption": "Table 2: (Upper) Dataset statistics. (Lower) Characteristics of MRAP in these datasets."}, {"id": "arXiv_pdf_2011_v2_0025@2011.05301-Table-4.jpg", "caption": "Table 3: Ablation study for MRAP. MAE measured on the experimental setup \u201850%\u2019."}, {"id": "arXiv_pdf_2011_v2_0025@2011.05301-Table-0.jpg", "caption": "Table 4: Performances on FB15K-237 with two different setup of observed node attribute sparsity"}, {"id": "arXiv_pdf_2011_v2_0025@2011.05301-Table-1.jpg", "caption": "Table 5: Performances on YAGO15K with two different setup of observed node attribute sparsity"}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Knowledge Graph Embeddings", "Knowledge Graph Completion", "Graph Classification", "Knowledge Graph Embedding", "Question Answering", "Knowledge Base Completion", "Graph Representation Learning", "Drug Discovery"], "datasets": ["DBpedia"], "queryEntities": {}}, {"id": "6a2dd445-f14b-3863-99d4-1f9e259b3ee2", "title": "Runtime Performances Benchmark for Knowledge Graph Embedding Methods", "score": 0.0, "url": "https://arxiv.org/abs/2011.04275", "pdfUrl": "https://arxiv.org/pdf/2011.04275.pdf", "bibTexUrl": null, "authors": ["Angelica Sofia Valeriani"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "This paper wants to focus on providing a characterization of the runtime performances of state-of-the-art implementations of KGE alghoritms, in terms of memory footprint and execution time. Despite the rapidly growing interest in KGE methods, so far little attention has been devoted to their comparison and evaluation; in particular, previous work mainly focused on performance in terms of accuracy in specific tasks, such as link prediction. To this extent, a framework is proposed for evaluating available KGE implementations against graphs with different properties, with a particular focus on the effectiveness of the adopted optimization strategies. Graphs and models have been trained leveraging different architectures, in order to enlighten features and properties of both models and the architectures they have been trained on....", "startOffset": 0, "endOffset": 840}], "summary": false}, {"title": "Introduction", "figure": null, "fragments": [{"end_offset": 784, "text": "Knowledge Graphs are structured representations of real world information. They have found many applications in industry and academic settings, and this brought to a considerable research effort towards large-scale information extraction from a variety of resources and question answering. A KG is a multi-relational graph composed of entities and relations. In KG, nodes represent entities, such as people and places, while edges define specific facts connecting two entities with a relation. In fact, each edge is represented as a triple of the form , meaning that two entities are connected by a specific relation. KGs can contain millions of entities and billions of facts; some examples can be Yago , FreeBase and WikiData .", "start_offset": 0}, {"end_offset": 1983, "text": "Second, embeddings are compressed representations, therefore they reduce memory utilization also of very huge graphs, allowing an easier managing.Third, vector operations are simpler and faster than comparable operations on graphs.", "start_offset": 1752}, {"end_offset": 2633, "text": "Given a KG, such techniques represent at first entities and relations in a continuous vector space, then define a scoring function on each fact to measure its plausibility.", "start_offset": 2461}, {"end_offset": 3850, "text": "Knowledge graph embedding methods are not only used for knowledge graph completion, but the learned embedding vectors of entities and relations are also very useful in a huge amount of emerging ML applications.", "start_offset": 3640}, {"end_offset": 6245, "text": "The chosen models were all taken considering the most representative models in their category, for example considering features like release time and number of citations.", "start_offset": 6075}], "summary": true}, {"title": "Overview on KGE Models & Models Choice", "figure": null, "fragments": [{"end_offset": 585, "text": "Knowledge Graphs such as WordNet , Freebase and Yago have been playing a fundamental role in many AI applications, such as relation extraction or question answering . They usually contain huge amounts of structured data as the form of triples , denoted as , where relation models the relationship between the two entities. In the last few years, many researchers focused on the knowledge graph completion task, which consists of predicting relations between entities based on existing triples in a knowledge graph .", "start_offset": 0}, {"end_offset": 935, "text": "Recently, a powerful approach for this task is to encode every element of a knowledge graph into a low-dimensional embedding vector space.", "start_offset": 772}, {"end_offset": 1380, "text": "These models translate the head entity embedding by summing with the relation embedding vector, then measuring distance between translated images of head entity and tail entity embedding.", "start_offset": 1193}, {"end_offset": 2622, "text": "Therefore, the solution to preserve the logical properties of relations in the embedding space is to distinguish the role of entities while embedding entities and relations.", "start_offset": 2449}, {"end_offset": 4960, "text": "Despite that, according to experimental results of previous works, TransE results competitive in terms of accuracy when compared to other KGE models, while obtaining outstanding performances in terms of time convergence .", "start_offset": 4733}, {"end_offset": 5620, "text": "In particular, convolutional neural networks use one or multiple convolutional layers: each of these layers performs convolution on the input data applying low-dimensional filters.", "start_offset": 5380}, {"end_offset": 5957, "text": "ConvKB is the most representative model in this category, with a high number of references in the literature, and it outperforms previous state-of-the-art models several benchmark link prediction datasets .", "start_offset": 5740}, {"end_offset": 8608, "text": "RESCAL is one of the earliest studies on matrix factorization based knowledge graph embedding models, using a bilinear form as score function, in the field of trilinear-product-based models.", "start_offset": 8413}], "summary": true}, {"title": "Methodology", "figure": 15, "fragments": [{"end_offset": 944, "text": "As anticipated in Section 2, the purpose of this project was to characterize the runtime performances of different KGE methods, w.r.t. the properties of input graphs and the adopted optimization strategies. To this aim, a python framework has been implemented to easily handle different KGE experiments, supporting multiple graphs, models, and configuration settings. Such framework has been built on top of AmpliGraph , an open source library for supervised learning on knowledge graphs. AmpliGraph is, in turn, based on TensorFlow , a state-of-the-art end-to-end open source platform for machine learning, widely employed both in industry and academia. The proposed python framework allows users to configure the execution of KGE experiments by setting a series of parameters. First, the user can select the KGE model to be employed in the experiment choosing among the three models presented in Section 2: TransE, DistMult and ConvKB.", "start_offset": 0}, {"end_offset": 3218, "text": "We set inter op parallelism threads = 2 for each experiment, according to reccomendations by Intel on how to maximize TensorFlow performance .", "start_offset": 3072}, {"end_offset": 3466, "text": "If the RAM usage is monitored, the peak RAM usage is saved both immediately after the load of the graph and at the end of the experiments.", "start_offset": 3328}, {"end_offset": 6208, "text": "The decision was to determine a suitable setting for all the KGE models and all the graphs; after a number of preliminary tests, and time passed studying this topic, the common setting was determined. The choice of selecting a common setting derives from the need of getting an efficient and consistent way to compare the performances results obtained by different models , across the investigated architectures.", "start_offset": 5767}], "summary": true}, {"title": "Experimental results", "figure": 0, "fragments": [{"end_offset": 759, "text": "In particular, Subsection 4.1 describes and comments the results obtained by test performing in a standard CPU environment, without vectorization enabled. Subsection 4.2 focuses on the improved environment, that is CPU environment with some vectorized instructions enables. Subsection 4.3 describes instead same tests trained and analyzed in the GPU environment. Finally, Subsection 4.4 makes a critical and complete analysis of all the tested architectures, commenting their features and obtained results. Considering the standard CPU architecture, tests have been performed with a number of threads of 8, 16 and 32, in case of the current architecture.", "start_offset": 105}, {"end_offset": 5194, "text": "Experimental results show that our model ConvKB outperforms other state-of-the-art models on two benchmark datasets WN18RR and FB15k-237 .", "start_offset": 5052}, {"end_offset": 6267, "text": "Same tests performed on CPU architecture without vectorization have been performed on CPU architecture with vectorization enabled, with a number of threads of 2, 4 and 8, due to architectural constraints.", "start_offset": 6063}, {"end_offset": 8651, "text": "The selected graphs, wn18, wn18rr, fb15k, fb15k-237 and yago3-10, have been trained with the most representative models of the main categories of models in the field of knowledge graph embedding; TransE, ConvKB and DistMult.", "start_offset": 8427}, {"end_offset": 10201, "text": "It is evident in all figures in this Section, how the CPU train time tends to increase considering an architecture with only a CPU, while it is constant in case of a GPU architecture. First of all, considering a CPU environment, it can be noticed that vectorization reduced the measured times, both for wall clock train time and CPU train time.", "start_offset": 9857}], "summary": true}, {"title": "Conclusions", "figure": null, "fragments": [{"end_offset": 640, "text": "The task of the project was to show the behavior of selected models in the process of knowledge graph embedding. On the whole, experimental results show the behavior of the models related to the specific graphs, according\nto the properties and the characteristics of the datasets. Furthermore, CPU architecture without vectorization, has shown that, varying the number of threads, wall clock time always decreases, as it measures the time taken to perform a job. On the contrary, CPU time always increases, as it measures the time in which the CPU is busy. This time can grow, due to the required amount for parallelization between threads.", "start_offset": 0}, {"end_offset": 1877, "text": "Besides, comparing the architectures, GPU proves to be the best architecture for the given task, both for execution time and RAM utilization, even if CPU with some vectorized instructions enabled still behaves well.", "start_offset": 1662}, {"end_offset": 2514, "text": "Finally, GPU has a total peak of RAM utilization that is inferior to CPU both with and without vectorization enabled.", "start_offset": 2397}, {"end_offset": 4140, "text": "Thanks go to professor Marco Domenico Santambrogio and PhD Guido Walter Di Donato from NECSTLab at Politecnico di Milano for supporting the development of this project.", "start_offset": 3972}, {"end_offset": 4838, "text": "Knowledge\nGraph Embedding: A Survey of Approaches and Applications\n Andrea Rossi, Donatella Firmani, Antonio Matinata, Paolo Merialdo,\nand Denilson Barbosa.", "start_offset": 4679}, {"end_offset": 6680, "text": "A Novel Embedding Model for Knowledge Base Completion Based on Convolutional Neural Network Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao and Li Deng.", "start_offset": 6519}], "summary": true}], "figures": [{"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-0.jpg", "caption": "Fig. 1. DistMult: comparison between CPU architecture without vectorization and with vectorization considering wall clock train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-7.jpg", "caption": "Fig. 10. TransE: comparison between CPU architecture with vectorization and GPU architecture considering CPU train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-5.jpg", "caption": "Fig. 11. ConvKB: comparison between CPU architecture with vectorization and GPU architecture considering wall clock train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-12.jpg", "caption": "Fig. 12. ConvKB: comparison between CPU architecture with vectorization and GPU architecture considering CPU train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-13.jpg", "caption": "Fig. 13. On the y-axis: RAM usage (in MegaBytes) for total peak and peak of loading graph. On the x-axis: all the tested graphs, considering a number 8 threads, with all the models. Architecture is CPU without vectorization."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-11.jpg", "caption": "Fig. 14. On the y-axis: RAM usage (in MegaBytes) for total peak and peak of loading graph. On the x-axis: all the tested graphs, considering a number 8 threads, with all the models. Architecture is CPU with vectorization."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-14.jpg", "caption": "Fig. 15. On the y-axis: RAM usage (in MegaBytes) for total peak and peak of loading graph. On the x-axis: all the tested graphs, considering a number 8 threads, with all the models. Architecture is GPU."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-4.jpg", "caption": "Fig. 2. DistMult: comparison between CPU architecture without vectorization and with vectorization considering CPU train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-3.jpg", "caption": "Fig. 3. TransE: comparison between CPU architecture without vectorization and with vectorization considering wall clock train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-1.jpg", "caption": "Fig. 4. TransE: comparison between CPU architecture without vectorization and with vectorization considering CPU train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-2.jpg", "caption": "Fig. 5. ConvKB: comparison between CPU architecture without vectorization and with vectorization considering wall clock train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-8.jpg", "caption": "Fig. 6. ConvKB: comparison between CPU architecture without vectorization and with vectorization considering CPU train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-9.jpg", "caption": "Fig. 7. DistMult: comparison between CPU architecture with vectorization and GPU architecture considering wall clock train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-6.jpg", "caption": "Fig. 8. DistMult: comparison between CPU architecture with vectorization and GPU architecture considering CPU train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Figure-10.jpg", "caption": "Fig. 9. TransE: comparison between CPU architecture with vectorization and GPU architecture considering wall clock train time (in seconds) on y-axis."}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Table-0.jpg", "caption": "Table 1. Knowledge Graph Properties"}, {"id": "arXiv_pdf_2011_v2_0016@2011.04275-Table-1.jpg", "caption": "Table 2. Employed Tools"}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Knowledge Graph Embeddings", "Knowledge Graph Completion", "Knowledge Graph Embedding", "Relation Extraction", "Stochastic Optimization", "Question Answering", "Knowledge Base Completion"], "datasets": [], "queryEntities": {}}, {"id": "7185077e-96b7-3852-935a-3fa1f692571b", "title": "COSMO: Conditional SEQ2SEQ-based Mixture Model for Zero-Shot Commonsense Question Answering", "score": 0.0, "url": "https://arxiv.org/abs/2011.00777", "pdfUrl": "https://arxiv.org/pdf/2011.00777.pdf", "bibTexUrl": null, "authors": ["Farhad Moghimifar", "Lizhen Qu", "Yue Zhuo", "Mahsa Baktashmotlagh", "Gholamreza Haffari"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Commonsense reasoning refers to the ability of evaluating a social situation and acting accordingly. Identification of the implicit causes and effects of a social context is the driving capability which can enable machines to perform commonsense reasoning. The dynamic world of social interactions requires context-dependent on-demand systems to infer such underlying information. However, current approaches in this realm lack the ability to perform commonsense reasoning upon facing an unseen situation, mostly due to incapability of identifying a diverse range of implicit social relations. Hence they fail to estimate the correct reasoning path. In this paper, we present Conditional SEQ2SEQ-based Mixture model (COSMO), which provides us with the capabilities of dynamic and diverse content generation....", "startOffset": 0, "endOffset": 810}], "summary": false}, {"title": "Introduction", "figure": 0, "fragments": [{"end_offset": 631, "text": "People understand narratives of everyday life by capitalising on their commonsense knowledge. They can easily reason about unobserved causes and effects in relation to the events described in narratives, as well as plausible characteristics and mental states of the involved persons. Although this kind of reasoning seems trivial for humans, it is still out of reach for current natural language understanding systems. Recently, there have been fast-growing interests in building AI systems with such human-like reasoning capabilities based on inferential commonsense knowledge .", "start_offset": 0}, {"end_offset": 1435, "text": "Knowledge-based approaches to such commonsense question-answering require an inferential knowledge base. ATOMIC is the largest commonsense knowledge base of this kind, which contains 300,000 short textual description of events and 877,000 typed if-then relations between events, categorised into 9 dimensions.", "start_offset": 1101}, {"end_offset": 1930, "text": "In particular, they encode the textual description of an event and a relation into an embedding, and maximise the probability of predicting the description of the associated event or characteristic of an involved person.", "start_offset": 1710}, {"end_offset": 2428, "text": "To address the above challenges, we propose a conditional SEQ2SEQ-based mixture model, named COSMO, to answer questions on everyday inferential knowledge in a zero-shot setting.", "start_offset": 2251}, {"end_offset": 4411, "text": "With the lack of training data in such question-answering settings, we devise a bespoken answer scoring module to assess the likelihood of each answer. Given a narrative and a question, the model generates sequences of plausible fact descriptions, as reasoning paths, by choosing different values of the latent variable.", "start_offset": 4091}], "summary": true}, {"title": "Related Works", "figure": null, "fragments": [{"end_offset": 748, "text": "Commonsense Knowledge Bases Introducing Commonsense Knowledge Graphs has provided a source of information for machines on the task of commonsense reasoning. This KGs, encode commonsense relations between different pair of events/concepts. Speer et al. assembled a KG from a variety of sources, ConceptNet, which represents general knowledge in form of tuples. While ConeptNet is more centered around taxonomic knowledge, Sap et al. constructed ATOMIC, a KG consisting inferential knowledge. Information in ATOMIC are presented as if-then relationship between events, mental states, and persona. The information in ATOMIC includes such information for the agent of the event and others, who might be affected.", "start_offset": 0}, {"end_offset": 1259, "text": "Although the presented KGs provide rich commonsense information, reasoning about social situation requires a dynamic on-demand approach for context-dependent information generation. Commonsense Knowledge Base Completion As an essential way of enabling machines to perform commonsense reasoning, the methods for automatic KG construction and completion have been studied recently.", "start_offset": 880}, {"end_offset": 2652, "text": "On the other hand, with the release of new commonsense KGs, such as ATOMIC and ConceptNet , the possibility of enriching language models with these KGs has been investigated vastly .", "start_offset": 2367}, {"end_offset": 4361, "text": "Puri improved the quality of unsupervised extractive question answering systems by introducing an approach involving answer generation, question generation and roundtrip filtration.", "start_offset": 4173}], "summary": true}, {"title": "Our Approach", "figure": null, "fragments": [{"end_offset": 396, "text": "In this section, we present our model COSMO for question answering in the zero-shot setting. In this setting, there is no training data for the QA task, thus we train our model only on ATOMIC to acquire inferential knowledge. We apply the trained model to answer multi-choice questions on everyday inferential knowledge by augmenting the trained model with a non-parametric answer scoring module.", "start_offset": 0}, {"end_offset": 2892, "text": "In the following, we will detail the KB module as well as its training procedure on ATOMIC, followed\nby presenting the answer scoring module and how to apply them together for the target task.", "start_offset": 2700}, {"end_offset": 3797, "text": "The key challenge of using pre-trained SEQ2SEQ models on ATOMIC is the diversity of output events based on a given event zi and relation r. The key idea herein is to align different latent factors hk with different outputs for the same input.", "start_offset": 3555}, {"end_offset": 4673, "text": "It takes as input a token sequence consisting of a latent variable value hk, a word sequence x, and a relation symbol r, and predicts a word sequence representing the next event z.", "start_offset": 4493}, {"end_offset": 8119, "text": "In this method, event prediction is considered as language modelling task, hence the score is defined as the average probability of generating each token of an event.", "start_offset": 7953}, {"end_offset": 8382, "text": "Based on the probabilistic model in Eq, the true answers should be semantically similar to the last events in plausible reasoning paths derived from contexts and questions.", "start_offset": 8205}, {"end_offset": 9292, "text": "The most plausible answer is the most probable reasoning path, whose last event achieves the highest similarity with the answer.", "start_offset": 9164}], "summary": true}, {"title": "Experiments", "figure": 1, "fragments": [{"end_offset": 170, "text": "In this section, we report the evaluation of our model on zero-shot question answering. To this end, we use test set and development set of SocialIQA .", "start_offset": 0}, {"end_offset": 595, "text": "In zero-hop setup, we only consider generating clauses using COSMO based on the context of the question, and the answers are then scored against the generated clauses. In one-hop setting, we take a step further and generate more clauses using the generated clauses in the previous step. This approach helps us uncover more implicit context-dependent information.", "start_offset": 233}, {"end_offset": 874, "text": "To further analyse the capability of our proposed model in terms of clause generation, we compare our model to other approaches on ATOMIC , and test and development set of SocialIQA.", "start_offset": 673}, {"end_offset": 3072, "text": "To further analyse the performance of our model in generation of clauses, we compare our model to the state-of-the-art automatic knowledge base completion model, that has been used in zero-shot commonsense question answering task, COMET .", "start_offset": 2811}, {"end_offset": 6472, "text": "The distance function (Eq. ) in our proposed model is evaluated with three variations of directly using probability of SEQ2SEQ model , BLEU function, and cosine distance.", "start_offset": 6290}, {"end_offset": 7715, "text": "The results of div bleu also shows that on ATOMIC and SocialIQA development and test set, our model outperforms all the baselines on all variation of BLEU function.", "start_offset": 7551}, {"end_offset": 7943, "text": "In this section, we demonstrate the capability of our model on diverse clause generation, and its effect on commonsense question answering.", "start_offset": 7804}], "summary": true}, {"title": "Conclusion", "figure": null, "fragments": [{"end_offset": 680, "text": "In this paper, we propose a novel approach for neuralising large-scale commonsense knowledge graph, Conditional SEQ2SEQ-based Mixture Model, COSMO. Our proposed model provides diverse clause generation, to ensure coverage for the target task. We use the proposed model to generate diverse context related clauses, alongside with our proposed answer classifier model, to address the task of zero-shot commonsense question answering task. Empirical results on zero-shot commonsense question answering dataset show the superiority of our model over the state-of-the-art methods, by up to 5.2%. Furthermore, our model outperforms baselines in terms of diversity of clause generations.", "start_offset": 0}], "summary": true}], "figures": [{"id": "arXiv_pdf_2011_v2_0001@2011.00777-Figure-0.jpg", "caption": "Figure 1: The illustration of the process of answering a question with our proposed model. The model receives a context and a question. Using Conditional SEQ2SEQ-based Mixture Model (COSMO), clauses based on the context and different relations are generated. The generated clauses are then used to generate new set of clauses based on the question. The Answer Classifier (AC) module selects the answer with higher score based on generated clauses at the last step and scores of COSMO. The path to the correct answer is distinguished with green color."}, {"id": "arXiv_pdf_2011_v2_0001@2011.00777-Table-2.jpg", "caption": "Table 1: The accuracy of answer prediction of our proposed model compared to the state-of-the-art models on SocialIQA, on development and test set."}, {"id": "arXiv_pdf_2011_v2_0001@2011.00777-Table-3.jpg", "caption": "Table 2: The results of using our proposed answer classifier function compared to the baselines, on development and test set of SocialIQA."}, {"id": "arXiv_pdf_2011_v2_0001@2011.00777-Table-4.jpg", "caption": "Table 3: The results of div ngram (top section) and div bleu(bottom section) of our model compared to the baselines, on test set of ATOMIC and SocialIQA."}, {"id": "arXiv_pdf_2011_v2_0001@2011.00777-Table-0.jpg", "caption": "Table 4: Examples of the test set of SocialIQA, with the clause generated by our proposed model, COSMO, compared to COMET."}, {"id": "arXiv_pdf_2011_v2_0001@2011.00777-Table-1.jpg", "caption": "Table 5: The templates that have been used to map the question from SocialIQA to the relations of ATOMIC, and phrases to use for language models, categorised by relation type (cause/effect/attribute). \u201cAGENT\u201d refers to the person who is the subject of the given context in the question answering tuple."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Combinatorial Optimization", "Transfer Learning", "Machine Translation", "Visual Question Answering", "Question Answering", "Knowledge Base Completion", "Language Modelling"], "datasets": [], "queryEntities": {}}, {"id": "7a0ed67d-d190-38c8-9488-33a5eb5f8ebb", "title": "Domain-specific Knowledge Graphs: A survey", "score": 0.0, "url": "https://arxiv.org/abs/2011.00235", "pdfUrl": "https://arxiv.org/pdf/2011.00235.pdf", "bibTexUrl": null, "authors": ["Bilal Abu-Salih"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Knowledge Graphs (KGs) have made a qualitative leap and effected a real revolution in knowledge representation. This is leveraged by the underlying structure of the KG which underpins a better comprehension, reasoning and interpreting of knowledge for both human and machine. Therefore, KGs continue to be used as a main driver to tackle a plethora of real-life problems in dissimilar domains. However, there is no consensus on a plausible and definition to domain KG. Further, in conjunction with several limitations and deficiencies, various domain KG construction approaches are far from perfection. This survey is the first to provide an inclusive definition to the notion of domain KG. Also, a comprehensive review of the state-of-the-art approaches drawn from academic works relevant to seven dissimilar domains of knowledge is provided....", "startOffset": 0, "endOffset": 846}], "summary": false}, {"title": "Generic Knowledge Graphs", "figure": null, "fragments": [{"end_offset": 623, "text": "In fact, since the invention of the Semantic Web, generic KGs have been associated with the Linked Data as being a natural representation for entities interlinking . Nevertheless, the term has gained the sheer momentum recently as it has fostered the computing paradigms by shifting from traditional databases to knowledge-bases . Ironically, there is no consensus on the definition of the term despite the few attempts to provide a reasonable description.", "start_offset": 159}, {"end_offset": 1197, "text": "A further technical depiction to the term is provided by Wang et al. in which the KG is conceived as a multidimensional graph encompassed of entities/nodes and relations/edges. Entities are interconnected using relations which are the edges of the graph, and facts are commonly represented as triples . Intuitively, two entities connected by a relation form a fact in the KG.", "start_offset": 789}, {"end_offset": 1679, "text": "Examples of various steadily evolving open-world KGs include: Freebase1, Semantically-Interlinked Online Communities , YAGO3, Dublin Core , Simple Knowledge Organization System , and DBPedia6 knowledge base.", "start_offset": 1453}], "summary": true}, {"title": "Domain-specific Knowledge Graphs", "figure": null, "fragments": [{"end_offset": 776, "text": "Despite the extensive use of the generic and open-world KGs to tackle a wide variety of domain-independent tasks, constructing KGs from domain corpora to benefit domain-specific problems is of high significance . This is because domain KGs have relevant and semantically interlinked applications with domain-specific problems. Intuitively, the notion of domain-specific KGs also lacks an agreement on an inclusive and a wellestablished definition considering that it still comparatively a new territory and an under-explored frontier . Nevertheless, some studies perceive the domain KG as a special type of KG that is used to represent a specific and complex domain . Others reported domain KGs as the process of enriching an underlying domain ontology .", "start_offset": 0}], "summary": true}, {"title": "Knowledge Graph Construction", "figure": null, "fragments": [{"end_offset": 202, "text": "KG was introduced as being an efficient and smart approach to tackle the continuous propagation of various forms of unstructured text and other structured or semi-structured sources .", "start_offset": 0}, {"end_offset": 1184, "text": "Various efforts have been undertaken in an attempt to tackle the KG construction problem.", "start_offset": 1095}], "summary": true}, {"title": "Knowledge Graph Embedding", "figure": null, "fragments": [{"end_offset": 588, "text": "KG Embedding is the process of creating propositional feature vector representations of the constituents of a KG so as to apply numeric techniques resulting scalable besides of effective . This is evident because KGE techniques can simplify resolving various complex real-life graph problems in which using conventional graph presentation is inadequate and inferior. This can be indicated in the current extensive use of KGE to tackle problems such as KG completion, entity recognition, and link-based clustering .", "start_offset": 0}, {"end_offset": 1067, "text": "The aim is to capture latent properties of the semantics in the KG so as alike entities and alike relations will be represented with similar vectors, and those not semantically connected are detached.", "start_offset": 867}, {"end_offset": 1822, "text": "Example of KGE models include, but not limited to; Translating Embedding , DistMult , Complex Embeddings , Holographic Embeddings , Convolutional 2D KG Embeddings , Convolution-based model , etc.", "start_offset": 1561}], "summary": true}, {"title": "Knowledge Graph Evaluation", "figure": null, "fragments": [{"end_offset": 252, "text": "The proliferation of massive constructed KGs poses a question on the quality of the embedded knowledge , and whether these facts do precisely depict the intended real-world concepts and interlinked via their relationships.", "start_offset": 0}, {"end_offset": 1152, "text": "7 8 9 10 \nIn domain KG, the absence of a complete and accurate KGs represents a challenge to the evaluation process. This is because collecting all true facts of a certain domain of interest is not a trivial task . Therefore, various attempts, commonly known as KG Augmentation/Completion techniques, have been undertaken to augment the KG with new facts depicted by new potential entities and/or new relations. To ensure data quality, these efforts are subject to correctness and completeness evaluation measures.", "start_offset": 496}, {"end_offset": 1608, "text": "These metrics are amongst various other measures that have been currently incorporated to evaluate the KG construction and completion in terms of the factuality of the embedded entities as well as their relations.", "start_offset": 1395}, {"end_offset": 1947, "text": "4. Domain-specific KGs This section reviews various domain-based KGs that were discussed in the literature. These domains are Healthcare, Education, ICT, Science and Engineering, Finance, Society and Politics, and Travel.", "start_offset": 1726}, {"end_offset": 2281, "text": "These tables demonstrate the specific KG usage, KG construction algorithm , the resources used to feed the KG, whether KG embedding techniques were incorporated, the evaluation approach, and the limitations of each designated work.", "start_offset": 2048}], "summary": true}, {"title": "Healthcare", "figure": null, "fragments": [{"end_offset": 857, "text": "Recently, Healthcare sector has gained much attention, particularly with coronavirus 2019 pandemic continues to rattle the world. Therefore, there is a notable consensus in both industry and academia to consolidate efforts to overcome the challenges of this vital sector . KGs offer the healthcare sector technical means to derive meaningful insights from voluminous and heterogeneous healthcare data . For example, Rotmensch et al. constructed a KG that captures diseases and symptoms related entities form 273,174 electronic medical records. The authors incorporate Google Health Knowledge Graph and created a KG that embodies diseases and symptoms and relationships between them. Rastogi et al. framed the personal health KG as a combination of context, personalization, and integration with other knowledge-bases.", "start_offset": 0}, {"end_offset": 2414, "text": "The authors proposed Semantics-based Data analytics framework that performs an exploratory and plausible analysis of the KG using plausible OWL extension and query rewriting algorithm.", "start_offset": 2222}, {"end_offset": 2849, "text": "Constructing a KG to benefit health management and to address current health-related problems and chronic diseases were proposed in the literature . For example, Huang et al. suggested a KG construction model that benefits people seeking knowledge regarding a healthy diet.", "start_offset": 2564}, {"end_offset": 5252, "text": "Application of KGs in healthcare and medical domains was demonstrated in other relevant tasks such as fraud, waste, and abuse Detection , drugs similarity , drug repurposing , clinical decision support systems , and medical recommender systems .", "start_offset": 4983}], "summary": true}, {"title": "Education", "figure": null, "fragments": [{"end_offset": 679, "text": "The construction and usage of educational KGs have been extended recently due to the significance of KGs application to the learning systems as well as the abundance of pedagogical data . Further, the KGs have proven ability to foster learning and been used in popular massive open online course platforms . Chen et al. presented K12EduKG, a KG constructed based on K-12 educational subjects. Domain-specific educational data was the source of knowledge that was used in K12EduKG. Concepts and relations are identified and imported into K12EduKG using CRF model and probabilistic association rule mining.", "start_offset": 0}, {"end_offset": 1062, "text": "Another attempt has been undertaken to build a KG that can be used to solve high school mathematical exercises including mathematical derivation and calculation .", "start_offset": 884}, {"end_offset": 1549, "text": "By using standard curriculum and learning assessment data as data sources, and by using neural network models for concepts and relations identification, KnowEdu is created to facilitate learner's cognitive and\neducational process.", "start_offset": 1319}, {"end_offset": 1939, "text": "Liu et al. and Lian et al. provided approaches that adopted graph-based relational learning for concept prerequisite learning in education domain.", "start_offset": 1783}, {"end_offset": 2367, "text": "The author proposed a KG that can conceptualise the internal control policy in the higher educational institutions.", "start_offset": 2252}, {"end_offset": 3384, "text": "With the use of three experimental KGs in the education domain, the authors demonstrated the significance of the proposed model when processing educational KGs.", "start_offset": 3224}], "summary": true}, {"title": "Ict", "figure": null, "fragments": [{"end_offset": 735, "text": "KGs have been widely used to benefit several applications related to Information and Communication Technology. In Cybersecurity, detecting and preventing cyberattack is inevitable to ensure providing continuous and uninterrupted services. Interestingly, various cybersecurity-related KGs have been introduced and developed. For example, [88] presented a practical approach for cybersecurity. They first developed a domain ontology that put forward a technique to construct the cybersecurity KG. Then they proposed a quintuple model that was used to obtain new knowledge using the path-ranking algorithm. Deng et al. discussed another cybersecurityrelated KG that was constructed to serve students who seek concepts in this domain.", "start_offset": 0}, {"end_offset": 1849, "text": "For example, Nayak et al. developed a KG that was used to extract test cases that would assist in the functional requirements gathering process.", "start_offset": 1700}, {"end_offset": 4176, "text": "These components are: the billing model, user access rights, network topology and application hierarchy, and the cable television operator network service model.", "start_offset": 4015}, {"end_offset": 4552, "text": "For example, Xie et al. proposed an IoT KG that was used in a new layer to map IoT devices, thereby unifying the communications of all devices.", "start_offset": 4403}], "summary": true}, {"title": "Sciences and Engineering", "figure": null, "fragments": [{"end_offset": 1106, "text": "Applying semantic web technologies and ontologies in natural sciences has proven successful leveraging the formal knowledge representation and the semantic web languages that can model rich and complex knowledge of natural sciences . For example, the utility of semantic analytics has been validated in providing a formal representation to chemical data, thereby increasing the sharing and interoperability of such data . Incorporating KG has extended these endeavours and provided a platform where information can be integrated from multiple chemical kinetic systems, and offered an automatic method to comprehend chemical mechanisms to perform complex chemical-related semantic queries . For example, [117] proposed an integrated system that used KG to demonstrate interoperability in cross-domain applications that compass combustion as well as to address the problem of data inconsistencies in chemical reaction mechanisms. Krdzavac et al. designed a domain ontology as an underlying structure of a KG that was used to demonstrate quantum chemistry calculations.", "start_offset": 0}, {"end_offset": 1695, "text": "Prior to the implementation of the incorporated KG embeddings, the authors constructed a domain KG that was populated with entities and relations captured from heterogeneous public data sources. These bio-related data sources are PubMed database11, Comparative\n11 \nToxicogenomics database 12, The Biological General Repository For Interaction datasets 13, and the human disease database: 14.", "start_offset": 1245}, {"end_offset": 2325, "text": "The authors made use of an integrated corpus composed of a geology dictionary and the Terminologies and Classification Codes of Geology and Mineral Resources to enrich the KG incorporating CRF-base geological word segmentation model.", "start_offset": 2077}, {"end_offset": 4168, "text": "For example, Fan et al. proposed an approach to construct the dispatch KG for the power grid to semantically describing behavior of dispatchers.", "start_offset": 4018}, {"end_offset": 4932, "text": "This is evident in various other engineering fields such as, nuclear engineering , marine engineering , Photonics engineering , Nanotechnology Engineering , Ceramics engineering , and Geomatics engineering .", "start_offset": 4695}], "summary": true}], "figures": [{"id": "arXiv_pdf_2011_v2_0005@2011.00235-Figure-0.jpg", "caption": "Figure 1. Number of studied papers per year."}, {"id": "arXiv_pdf_2011_v2_0005@2011.00235-Table-5.jpg", "caption": "Table A.1: Overview of KG approaches in healthcare domain"}, {"id": "arXiv_pdf_2011_v2_0005@2011.00235-Table-2.jpg", "caption": "Table A.2: Overview of KG approaches in education domain"}, {"id": "arXiv_pdf_2011_v2_0005@2011.00235-Table-4.jpg", "caption": "Table A.3: Overview of KG approaches in ICT domain"}, {"id": "arXiv_pdf_2011_v2_0005@2011.00235-Table-6.jpg", "caption": "Table A.4: Overview of KG approaches in science and engineering domain"}, {"id": "arXiv_pdf_2011_v2_0005@2011.00235-Table-0.jpg", "caption": "Table A.5: Overview of KG approaches in Finance domain"}, {"id": "arXiv_pdf_2011_v2_0005@2011.00235-Table-1.jpg", "caption": "Table A.6: Overview of KG approaches in the social science domain"}, {"id": "arXiv_pdf_2011_v2_0005@2011.00235-Table-3.jpg", "caption": "Table A.7: Overview of KG approaches in Travel domain"}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Knowledge Graph Embeddings", "Decision Making", "Entity Extraction", "Knowledge Graph Completion", "Learning-to-Rank", "Named Entity Recognition", "Stock Price Prediction", "Abuse Detection", "Network Embedding", "Relation Extraction", "Stochastic Optimization", "Question Answering", "Entity Alignment", "Sentiment Analysis", "Relational Reasoning", "Active Learning", "Stock Trend Prediction", "Knowledge Graph Embedding", "Stock Prediction", "Automated Software Engineering", "Open Information Extraction", "Knowledge Base Completion", "Machine Reading Comprehension", "Fraud Detection"], "datasets": ["Social media", "DBpedia"], "queryEntities": {}}, {"id": "b6ba1cca-8d4b-3c3e-8a3d-445de3310f1c", "title": "AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts", "score": 0.0, "url": "https://arxiv.org/abs/2010.15980", "pdfUrl": "https://arxiv.org/pdf/2010.15980.pdf", "bibTexUrl": null, "authors": ["Taylor Shin", "Yasaman Razeghi", "Robert L. Logan", "Eric Wallace", "Sameer Singh"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "The remarkable success of pretrained language models has motivated the study of what kinds of knowledge these models learn during pretraining. Reformulating tasks as fill-in-the-blanks problems (e.g., cloze tests) is a natural approach for gauging such knowledge, however, its usage is limited by the manual effort and guesswork required to write suitable prompts. To address this, we develop AutoPrompt, an automated method to create prompts for a diverse set of tasks, based on a gradient-guided search. Using AutoPrompt, we show that masked language models (MLMs) have an inherent capability to perform sentiment analysis and natural language inference without additional parameters or finetuning, sometimes achieving performance on par with recent state-of-the-art supervised models. We also show that our prompts elicit more accurate factual knowledge from MLMs than the manually created prompts on the LAMA benchmark, and that MLMs can be used as relation extractors more effectively than supervised relation extraction models....", "startOffset": 0, "endOffset": 1036}], "summary": false}, {"title": "Introduction", "figure": 0, "fragments": [{"end_offset": 349, "text": "Pretrained language models have had exceptional success when adapted to downstream tasks via finetuning . Although it is clear that pretraining improves accuracy, it is difficult to determine whether the knowledge that finetuned LMs contain is learned during the pretraining or the finetuning process.", "start_offset": 0}, {"end_offset": 1713, "text": "Attention visualization, another common technique, has a similar failure mode: attention scores may be correlated with, but not caused by the underlying target knowledge, leading to criticism against their use as explanations . Both probing and attention visualizations also struggle to evaluate knowledge that cannot be represented as simple token- or sequencelevel classification tasks. A more direct approach for eliciting knowledge from these models, since they are language models after all, is prompting, i.e. converting tasks into a language model format.", "start_offset": 1099}, {"end_offset": 2015, "text": "Similarly, Petroni et al. manually reformulate a knowledge base completion task as a cloze test .", "start_offset": 1876}, {"end_offset": 2719, "text": "Not only is this time consuming and non-intuitive for many tasks , more importantly, models are highly sensitive to this context: improperly-constructed contexts cause artificially low performance .", "start_offset": 2475}, {"end_offset": 3286, "text": "The same set of trigger tokens is used for all inputs, and is learned using a variant of the gradient-based search strategy proposed in Wallace et al.", "start_offset": 3128}, {"end_offset": 3771, "text": "First, we use AUTOPROMPT to construct prompts that test pretrained masked language models on sentiment analysis and natural language inference .", "start_offset": 3615}, {"end_offset": 4911, "text": "Finally, although the goal of AUTOPROMPT is to analyze models, we find that it provides certain practical advantages over finetuning.", "start_offset": 4778}], "summary": true}, {"title": "Overview of AUTOPROMPT", "figure": null, "fragments": [{"end_offset": 101, "text": "A natural way to elicit knowledge from pretrained LMs is to pose tasks as fill-in-the-blank problems.", "start_offset": 0}, {"end_offset": 582, "text": "In light of this, we introduce AUTOPROMPT, a method that constructs customized prompts for a specific task and MLM of interest, to cause the MLMs to produce the desired knowledge.1. An illustration of AUTOPROMPT is provided in Figure 1.", "start_offset": 347}, {"end_offset": 1202, "text": "In the following sections, we describe how AUTOPROMPT uses labeled training data to construct prompts, and how it uses the output of the MLM as a prediction for the task. For the purpose of prompt construction, we distinguish the original task inputs xinp from the prompt xprompt that is fed into the MLM.", "start_offset": 782}, {"end_offset": 1936, "text": "If class labels naturally correspond to tokens in the vocabulary , this distribution may be readily interpreted as a distribution over class labels.", "start_offset": 1733}, {"end_offset": 3651, "text": "Note that computing this candidate set is roughly as expensive as a single forward pass and backward pass of the model .", "start_offset": 3432}, {"end_offset": 4200, "text": "While in some settings the choice of label tokens is obvious , it is less clear what label tokens are appropriate for problems involving more abstract class labels .", "start_offset": 3952}, {"end_offset": 7078, "text": "At the end of every iteration, we measure the label likelihood on withheld development data, and return the best prompt found during the entire search as the final output.", "start_offset": 6907}, {"end_offset": 7489, "text": "Our AUTOPROMPT implementation is publicly available at and supports prompt generation for pretrained models in the HuggingFace transformers library on arbitrary datasets.", "start_offset": 7263}], "summary": true}, {"title": "Sentiment Analysis", "figure": 5, "fragments": [{"end_offset": 464, "text": "Sentiment analysis is a fundamental task in NLP, both for natural language understanding research and real-world applications. It is also difficult to probe the extent to which MLMs understand sentiment without finetuning. Setup We apply our method to convert instances from the binary Stanford Sentiment Treebank into prompts, using the standard train/test splits. We find label tokens using a prompt based on the template in Table 3.", "start_offset": 0}, {"end_offset": 857, "text": "All prompts are initialized with the same template used to find the label set. We also construct a prompt manually based on the intuition that SST-2 is comprised of movie reviews.", "start_offset": 622}, {"end_offset": 1538, "text": "Results We show results in Table 1, along with reference scores from the GLUE SST-2 leaderboard, and scores for a linear probe trained over the elementwise average of the LM token representations. Prompts generated by AUTOPROMPT reveal that both BERT and RoBERTa have a strong knowledge of sentiment analysis: without any finetuning, BERT per3Required 2 days to run with 8 NVIDIA 2080Ti GPUs. forms comparably to a supervised BiLSTM, and RoBERTa achieves an accuracy on-par with finetuned BERT and ELMo models.", "start_offset": 1008}, {"end_offset": 2205, "text": "In particular, we measure the development set accuracy of AUTOPROMPT prompts when using random subsets of 10, 100, and 1000 instances from the training data.", "start_offset": 2048}], "summary": true}, {"title": "Natural Language Inference", "figure": 1, "fragments": [{"end_offset": 737, "text": "To evaluate the semantic understanding of MLMs, we experiment on Natural Language Inference . NLI is crucial in many tasks such as reading comprehension and commonsense reasoning , and it is used as a common benchmark for language understanding. Setup We use the entailment task from the SICK dataset which consists of around 10,000 pairs of human-annotated sentences labeled as entailment, contradiction, and neutral. The standard dataset is biased toward the neutral class which represent 56.7% of instances. We also experiment on an unbiased variant with 2-way classification of contradiction vs. entailment , as well as an unbiased 3-way classification variant .", "start_offset": 0}, {"end_offset": 1165, "text": "Results Table 2 shows that AUTOPROMPT considerably outperforms the majority baseline in all experiments. For example, on the 2-way SICK-E dataset, AUTOPROMPT is comparable to a supervised finetuned BERT.", "start_offset": 962}, {"end_offset": 1516, "text": "Overall, these results demonstrate that both BERT and RoBERTa have some inherent knowledge of natural language inference.", "start_offset": 1395}, {"end_offset": 1804, "text": "The results in Figure 2 show that AUTOPROMPT performs on par with finetuned BERT and significantly better than finetuned RoBERTa in low data settings.", "start_offset": 1654}, {"end_offset": 2412, "text": "These results suggest that AUTOPROMPT may be more accurate for concepts that can be easily expressed using natural label tokens.", "start_offset": 2284}], "summary": true}, {"title": "Fact Retrieval", "figure": 9, "fragments": [{"end_offset": 286, "text": "An important question is whether pretrained MLMs know facts about real-world entities. The LAMA dataset evaluates this using cloze tests that consist of (sub, rel, obj) triples, e.g, and manually created prompts with missing objects, e.g.", "start_offset": 0}, {"end_offset": 723, "text": "LPAQA extends this idea by systematically creating prompts that are generated by mining Wikipedia, paraphrasing, and crowdsourcing. In this section, we use the same cloze-style setup but automatically generate prompts in order to better evaluate the factual knowledge of MLMs. We compare our approach against LAMA and LPAQA, which are explicitly designed for the task of fact retrieval.", "start_offset": 316}, {"end_offset": 953, "text": "where the trigger tokens are specific to the relation rel and the correct object obj is the label token.", "start_offset": 849}, {"end_offset": 1191, "text": "To collect training data for AUTOPROMPT, we gather at most 1000 facts for each of the 41 relations in LAMA from the T-REx dataset .", "start_offset": 1038}, {"end_offset": 1426, "text": "We ensure that none of the T-REx triples are present in the test set, and we split the data 80-20 into train and development sets.", "start_offset": 1296}, {"end_offset": 3283, "text": "Overall, these results show that AUTOPROMPT can retrieve facts more effectively than past prompting methods, thus demonstrating that BERT contains more factual knowledge than previously estimated.", "start_offset": 3087}, {"end_offset": 4438, "text": "BERT actually slightly outperforms RoBERTa, and we find that the prompts generated for RoBERTa tend to contain more irrelevant words .", "start_offset": 4279}], "summary": true}, {"title": "Relation Extraction", "figure": 7, "fragments": [{"end_offset": 125, "text": "Apart from evaluating whether MLMs know facts, it is also important to evaluate whether they can extract knowledge from text.", "start_offset": 0}, {"end_offset": 961, "text": "where the trigger tokens are specific to the relation, and label token is the correct object obj . Setup We use the T-Rex dataset for RE because each T-REx fact comes with context sentences that mention the subject and object surface forms. We compare AUTOPROMPT to LAMA and LPAQA , as well as a recent supervised relation extraction model that was also used by Petroni et al.", "start_offset": 483}, {"end_offset": 1944, "text": "MLMs can extract relational information more effectively than the supervised RE model, providing up to a 33% increase on the task when using AUTOPROMPT.", "start_offset": 1792}, {"end_offset": 2247, "text": "For both BERT and RoBERTa, we notice that the trigger tokens consist of words related to their corresponding relations , e.g.", "start_offset": 2083}, {"end_offset": 2508, "text": "Perturbed Sentence Evaluation A possible explanation for the strong results of MLMs in the RE setting is that they may already know many of the relations.", "start_offset": 2354}, {"end_offset": 3178, "text": "We regenerate the prompts using the perturbed version of the data. The accuracy of the RE model does not change significantly on the perturbed data , however, the accuracy of the MLMs decreases significantly.", "start_offset": 2961}, {"end_offset": 3456, "text": "Nevertheless, our prompts for BERT outperform their LAMA and LPAQA counterparts, which provides further evidence that AUTOPROMPT produces better probes.", "start_offset": 3304}], "summary": true}, {"title": "Discussion", "figure": null, "fragments": [{"end_offset": 380, "text": "Prompting as an Alternative to Finetuning The goal of prompting a language model is to probe the knowledge that the model acquired from pretraining. Nevertheless, prompting has some practical advantages over finetuning for solving realworld tasks. First, as shown in Section 3, prompts generated using AUTOPROMPT can achieve higher accuracy than finetuning in the low-data regime.", "start_offset": 0}, {"end_offset": 786, "text": "In particular, finetuning requires storing large language model checkpoints for each individual task, and, more importantly, it drastically increases system cost and complexity because it requires deploying many different models at the same time.", "start_offset": 540}, {"end_offset": 1280, "text": "Limitations of Prompting There are certain phenomena that are difficult to elicit from pretrained language models via prompts. In our preliminary evaluation on datasets such as QQP and RTE , prompts generated manually and with AUTOPROMPT did not perform considerably better than chance.", "start_offset": 954}, {"end_offset": 1626, "text": "In general, different probing methods have different tasks and phenomena they are suitable for: AUTOPROMPT makes prompt-based probes more generally applicable, but, it still remains just one tool in the toolbox of the interpretability researcher.", "start_offset": 1380}, {"end_offset": 2061, "text": "Although this is also required for other probing techniques , manual prompts rely on domain/language insights instead of labeled data. Compared to human-designed prompts, AUTOPROMPT generated prompts lack interpretability, which is similar to other probing techniques, such as linear probing classifiers.", "start_offset": 1723}, {"end_offset": 2530, "text": "Finally, due to the greedy search over the large discrete space of phrases, AUTOPROMPT is sometimes brittle; we leave more effective crafting techniques for future directions.", "start_offset": 2355}], "summary": true}, {"title": "Conclusion", "figure": null, "fragments": [{"end_offset": 1161, "text": "In this paper, we introduce AUTOPROMPT, an approach to develop automatically-constructed prompts that elicit knowledge from pretrained MLMs for a variety of tasks. We show that these prompts outperform manual prompts while requiring less human effort. Furthermore, the results for sentiment analysis and textual entailment suggest that, in some data-scarce settings, it may be more effective to prompt language models than to finetune them for the task. Although we focus only on masked language models in this paper, our method can be trivially extended to standard language models, and thus maybe useful for constructing inputs for models like GPT-3 . Source code and datasets to reproduce the results in this paper is available at Acknowledgments. We would like to thank the LAMA and LPAQA teams for answering our questions. We would also like to thank the members of UCI NLP, Matt Gardner, Sebastian Riedel, and Antoine Bosselut for valuable feedback. This material is based upon work sponsored by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research.", "start_offset": 0}], "summary": true}, {"title": "B Relation Extraction Details", "figure": null, "fragments": [{"end_offset": 459, "text": "Following Petroni et al, we use the pretrained RE model from Sorokin and Gurevych as our baseline. To encode the sentence, this model uses a combination of an LSTM-based relation encoder and an attention mechanism. To make predictions, the model constructs a knowledge graph whose edges are the extracted relation triples. The standard RE evaluation measures how well the model predicts the relation types of entity pairs on the sentence level.", "start_offset": 0}, {"end_offset": 730, "text": "We feed the RE model sentences from test facts and we query the resulting graph for all edges that contain the given subject and relation.", "start_offset": 592}, {"end_offset": 1002, "text": "We do this for every fact and take the average across all relations to get the overall precision. The RE model is not trained to predict two of the original T-REx relations.", "start_offset": 829}], "summary": true}], "figures": [{"id": "arXiv_pdf_2010_v2_0083@2010.15980-Figure-1.jpg", "caption": "Figure 1: Illustration of AUTOPROMPT applied to probe a masked language model\u2019s (MLM\u2019s) ability to perform sentiment analysis. Each input, xinp, is placed into a natural language prompt, xprompt, which contains a single [MASK] token. The prompt is created using a template, \u03bb, which combines the original input with a set of trigger tokens, xtrig. The trigger tokens are shared across all inputs and determined using a gradient-based search (Section 2.2). Probabilities for each class label, y, are then obtained by marginalizing the MLM predictions, p([MASK]|xprompt), over sets of automatically detected label tokens (Section 2.3)."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Figure-2.jpg", "caption": "Figure 2: Effect of Training Data on sentiment analysis and NLI for AUTOPROMPT vs. finetuning. X-axis is the number of data points used during training. Error bars plot the max. and min. accuracies observed over 10 independent runs. In the low data regime, AUTOPROMPT outperforms finetuning for RoBERTa."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Figure-0.jpg", "caption": "Figure 3: Effect of Label and Trigger Set Sizes on sentiment analysis. The number of candidate replacements is fixed at |Vcand| = 100. Increasing the label set size improves performance, while changing the trigger length does not have much impact."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Table-7.jpg", "caption": "Table 1: Sentiment Analysis performance on the SST2 test set of supervised classifiers (top) and fill-in-theblank MLMs (bottom). Scores marked with \u2020 are from the GLUE leaderboard: http://gluebenchmark.com/ leaderboard."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Table-0.jpg", "caption": "Table 2: Natural Language Inference performance on the SICK-E test set and variants. (Top) Baseline classifiers. (Bottom) Fill-in-the-blank MLMs."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Table-2.jpg", "caption": "Table 3: Example Prompts by AUTOPROMPT for each task. On the left, we show the prompt template, which combines the input, a number of trigger tokens [T], and a prediction token [P]. For classification tasks (sentiment analysis and NLI), we make predictions by summing the model\u2019s probability for a number of automatically selected label tokens. For fact retrieval and relation extraction, we take the most likely token predicted by the model."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Table-1.jpg", "caption": "Table 4: Factual Retrieval: On the left, we evaluate BERT on fact retrieval using the Original LAMA dataset from Petroni et al. (2019). For all three metrics (mean reciprocal rank, mean precision-at-10 (P@10), and mean precision-at-1(P@1)), AUTOPROMPT significantly outperforms past prompting methods. We also report results on a T-REx version of the data (see text for details). On the right, we compare BERT versus RoBERTa on a subset of the LAMA data using AUTOPROMPT with 5 tokens."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Table-6.jpg", "caption": "Table 5: Relation Extraction: We use prompts to test pretrained MLMs on relation extraction. Compared to a state-of-the-art LSTM model from 2017, MLMs have higher mean precision-at-1 (P@1), especially when using prompts from AUTOPROMPT. We also test models on sentences that have been edited to contain incorrect facts. The accuracy of MLMs drops significantly on these sentences, indicating that their high performance stems from their factual knowledge."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Table-5.jpg", "caption": "Table 6: A breakdown of all relations for fact retrieval on the original dataset from Petroni et al. (2019). We compare P@1 of prompts generated by LAMA, LPAQA, and our approach using five prompt tokens."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Table-4.jpg", "caption": "Table 7: Examples of manual prompts (first line, shown with BERT\u2019s P@1) and prompts generated via AUTOPROMPT for Fact Retrieval."}, {"id": "arXiv_pdf_2010_v2_0083@2010.15980-Table-3.jpg", "caption": "Table 8: Examples of prompts generated using AUTOPROMPT for relation extraction. Underlined words represent the gold object. The bottom half of the Table shows examples of our augmented evaluation where the original objects (represented by crossed-out words) are replaced by new objects."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Relation Extraction", "Reading Comprehension", "Natural Language Inference", "Knowledge Base Completion", "Question Answering", "Sentiment Analysis"], "datasets": ["GLUE", "SST", "SUBJ"], "queryEntities": {}}, {"id": "2db2440a-f7b9-3223-aa36-f0afbb8b01a9", "title": "A Survey of Embedding Space Alignment Methods for Language and Knowledge Graphs", "score": 0.0, "url": "https://arxiv.org/abs/2010.13688", "pdfUrl": "https://arxiv.org/pdf/2010.13688.pdf", "bibTexUrl": null, "authors": ["Alexander Kalinowski", "Yuan An"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Neural embedding approaches have become a staple in the fields of computer vision, natural language processing, and more recently, graph analytics. Given the pervasive nature of these algorithms, the natural question becomes how to exploit the embedding spaces to map, or align, embeddings of different data sources. To this end, we survey the current research landscape on word, sentence and knowledge graph embedding algorithms. We provide a classification of the relevant alignment techniques and discuss benchmark datasets used in this field of research. By gathering these diverse approaches into a singular survey, we hope to further motivate research into alignment of embedding spaces of varied data types and sources.", "startOffset": 0, "endOffset": 726}], "summary": false}, {"title": "Introduction", "figure": null, "fragments": [{"end_offset": 347, "text": "The purpose of this survey is to explore the core techniques and categorizations of methods for aligning low-dimensional embedding spaces. Projecting sparse, high-dimensional data sets into compact, lower-dimensional spaces allows not only for a significant reduction in storage space, but also builds dense representations with many applications.", "start_offset": 0}, {"end_offset": 1171, "text": "In order to fully leverage these dense representations and translate them across domains and problem spaces, techniques for establishing alignments between them must be developed and understood.", "start_offset": 977}, {"end_offset": 1845, "text": "In machine learning, however, methods for learning from this data are skewed to those of supervised learning where labeled data instances are required to optimize models and generalize to new applications.", "start_offset": 1640}, {"end_offset": 3224, "text": "We believe that alignment methods provide a convenient method for deriving correspondences between pairs of embedding spaces, thereby providing a method for bootstrapping fuzzy labels between the source and target spaces.", "start_offset": 3003}, {"end_offset": 4072, "text": "To avoid building these hand-labeled datasets, the task of Bilingual Lexical Induction aims to learn mappings from a source to target language in an unsupervised or semi-supervised manner . A critical task in machine translation systems, BLI has been influenced heavily by embedding techniques, beginning with linear maps from one embedding space to another .", "start_offset": 3696}, {"end_offset": 4948, "text": "An ontology is a formal specification of the types of objects and relationships between those objects in a given domain.", "start_offset": 4828}, {"end_offset": 6075, "text": "Knowledge graphs explicitly frame ontologies using graph data types, allowing for advanced data representation algorithms, such as graph embeddings, to be applied for a variety of tasks, including knowledge base completion .", "start_offset": 5848}, {"end_offset": 7629, "text": "Harmonizing free text input and understanding with information in a knowledge graph can be seen as an alignment between these two resources, helping to improve the usability and accuracy of these systems.", "start_offset": 7425}], "summary": true}, {"title": "Embedding Models", "figure": null, "fragments": [{"end_offset": 612, "text": "In this section, we outline the basic methods for embedding language and knowledge data into low-dimensional vector spaces. Words can be viewed as an atomic unit of natural language. Taking this viewpoint, creating features for machine learning models that involve language can be time consuming. These features typically can include one-hot representations of words or counts of words involved in a sentence or document, both of which suffer from high-dimensionality and sparsity. Finding dense, lower-dimensional representations of words to replace traditional features is the focus of work on word embeddings.", "start_offset": 0}, {"end_offset": 944, "text": "By training such a network, co-occurrences between words are learned to project like-words into the same areas of the low-dimensional space, reflecting their syntactic and semantic properties.", "start_offset": 752}, {"end_offset": 3897, "text": "Beginning with sub-word embeddings trained using FastText, an efficient sentence classification model is established in [16] by representing a sentence as the average of its component word representations.", "start_offset": 3692}, {"end_offset": 7780, "text": "Coupled with a bi-directional LSTM model, the InferSent model can be pre-trained on natural language inference tasks such as sentence semantic similarity and later used for inference or fine-tuning on other tasks.", "start_offset": 7561}, {"end_offset": 8244, "text": "Contextual models such as BERT provide an encoding of each positional word in an input sentence as their output, thus it is necessary to aggregate these contextualized representations into a single static sentence representation.", "start_offset": 8015}, {"end_offset": 17458, "text": "Translation-based methods, semantic-matching models and graph-structure models have all been used to embed individual knowledge graphs as well as aide in entity alignment between embedded graphs, as described in Section 3.4.", "start_offset": 17234}], "summary": true}, {"title": "Alignment Approaches", "figure": null, "fragments": [{"end_offset": 303, "text": "Abstractly, learning a mapping function between two vector spaces is a well studied problem. Let D1 and D2 be two datasets, originating from either similar or different domains.", "start_offset": 0}, {"end_offset": 900, "text": "These similarities are measured in the lower-dimensional vector spaces through techniques such as, but not limited to, Euclidean distance or cosine similarity.", "start_offset": 741}, {"end_offset": 3277, "text": "We then measured the trend over time for mentions of word embedding alignments, sentence embedding alignments and knowledge graph embedding alignments.", "start_offset": 3126}, {"end_offset": 4758, "text": "In these cases, we assume that the direction of the learned alignment mapping is irrelevant, i.e. we could easily reverse the source and target spaces and learn an alignment map in the reverse direction.", "start_offset": 4555}, {"end_offset": 6514, "text": "Moving past semi-supervised methods, approaches to learning mappings between embedding spaces in a completely unsupervised way.", "start_offset": 6387}, {"end_offset": 9202, "text": "The research area of sentence-to-sentence alignment relies on parallel documents, typically found in resources such as translations of the European Parliament proceedings or the Bible.", "start_offset": 9018}, {"end_offset": 10175, "text": "They also serve as the backbone to the Semantic Web, a set of standards for defining and linking data and meta-data in a machine-readable and human-interpretable way.", "start_offset": 10009}, {"end_offset": 11215, "text": "Work in this area originated in the task of ontology alignment , which aimed to use heuristics, string matching and natural language processing techniques to map source and target nodes.", "start_offset": 11025}, {"end_offset": 12460, "text": "One such way of finding these correspondences is to find a map between the token embedding f1 and the entity embedding f2 .", "start_offset": 12332}], "summary": true}, {"title": "Alignment Learning Paradigms", "figure": null, "fragments": [{"end_offset": 1022, "text": "In this section, we explore the major techniques used in each of the six categories defined above. Each section reviews the works from the perspective of classifying them into supervised, semi-supervised and unsupervised frameworks, motivated by our desire to assess the requisite amount of parallel data necessary to learn an alignment. We proceed by classifying word-to-word alignment techniques into supervised, semi-supervised and unsupervised methods. Supervised learning methods are the most common and most data intensive in machine learning applications. To help alleviate the burden on developers of these methods, leveraging unsupervised methods as discussed above helps to ingest large amounts of data and build robust features to jumpstart learning. In this section we discuss supervised models that use unsupervised features as inputs with the goal of aligning these resources. Regression Models Regression models form the class of solutions first used to address the word-to-word embedding alignment problem.", "start_offset": 0}, {"end_offset": 15236, "text": "To address issues of coverage in cross-lingual knowledge graphs, [60] propose a method for embedding knowledge graphs in a source and target language and automatically learning alignments between them, called MTransE.", "start_offset": 15019}, {"end_offset": 29392, "text": "By embedding the separate graphs using translational-based methods, the authors of [70] build a joint embedding space and utilize a soft alignment scoring function to estimate a reliability score of aligned entities.", "start_offset": 29176}, {"end_offset": 37126, "text": "In addition to defining a piece of the loss function for sentence representation pairs that are in the training set, the authors also use all available non-parallel data in their approach.", "start_offset": 36931}], "summary": true}, {"title": "Benchmark Datasets", "figure": 1, "fragments": [{"end_offset": 277, "text": "In this section, we document some of the most popular datasets used in the alignment literature. As introduced in Section 1.1, the task of Bilingual Lexical Induction aims to evaluate the consistency and ability to learn alignments between embeddings of two distinct languages.", "start_offset": 0}, {"end_offset": 809, "text": "In regards to the latter, such published datasets typically select a base embedding algorithm and generate pre-trained embeddings on several monolingual corpora, each using the same model hyper-parameters to dictate consistency.", "start_offset": 556}, {"end_offset": 1215, "text": "This dataset contains monolingual embeddings and seed dictionaries for 30 languages, as well as bilingual seed dictionary pairs for 110 languages. For languages coupled with monolingual embeddings, all such embeddings were generated using the FastText algorithm trained over a copy of Wikipedia in the respective language.", "start_offset": 893}, {"end_offset": 1604, "text": "While this paradigm allows for consistency in comparing the embedding spaces, it could be the case that the embeddings may perform better on certain languages, confounding the evaluation of the structural similarities between embedding spaces.", "start_offset": 1361}, {"end_offset": 2359, "text": "This dataset was compiled automatically from Europarl , a compilation of European parliament proceedings in 21 languages, typically packaged for evaluation systems into four primary languages: English, Finnish, German and Spanish .", "start_offset": 2120}, {"end_offset": 2772, "text": "Benchmarking experiments on knowledge graph entity alignment typically required two or more source knowledge graphs with a degree of known overlapping entities.", "start_offset": 2612}, {"end_offset": 3050, "text": "The WK31 datasets are an example of this paradigm, containing knowledge graphs focusing on the DBPedia person domain across English, French and German.", "start_offset": 2899}, {"end_offset": 3336, "text": "These datasets are additionally evaluated based on the number of inter-lingual links , where the ILL identify the same entity across two pairs of languages.", "start_offset": 3175}], "summary": true}, {"title": "Summary and Open Questions", "figure": null, "fragments": [{"end_offset": 190, "text": "To summarize, we conducted a survey of the literature on the task of aligning diverse embedding spaces output by various neural networks for creating low-dimensional representations of data.", "start_offset": 0}, {"end_offset": 506, "text": "The majority of these methods aim to learn alignment models between spaces of the same underlying data type, i.e. words-to-words or graphs-to-graphs, typically with the alignment meant to bridge the gap between languages.", "start_offset": 285}, {"end_offset": 971, "text": "By identifying this gap and outlining existing methodologies we hope this survey provides an entry point for other researchers with a shared goal of aligning embedding spaces of diverse data types.", "start_offset": 774}], "summary": true}], "figures": [{"id": "arXiv_pdf_2010_v2_0072@2010.13688-Table-3.jpg", "caption": "Table 1: Keyword Labels for Research Classification"}, {"id": "arXiv_pdf_2010_v2_0072@2010.13688-Table-0.jpg", "caption": "Table 2: Seed Alignments of WK31 Datasets"}, {"id": "arXiv_pdf_2010_v2_0072@2010.13688-Table-1.jpg", "caption": "Table 3: Overlapping Alignments of DFB Datasets"}, {"id": "arXiv_pdf_2010_v2_0072@2010.13688-Table-2.jpg", "caption": "Table 4: Metrics of DFP Datasets"}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Knowledge Graph Embeddings", "Entity Extraction", "Named Entity Recognition", "Transfer Learning", "Machine Translation", "Relation Extraction", "Part-of-Speech Tagging", "Natural Language Inference", "Question Answering", "Graph Representation Learning", "Entity Alignment", "Sentence Classification", "Sentence Embedding", "Semantic Role Labeling", "Knowledge Graph Embedding", "Cross-Lingual Natural Language Inference", "Knowledge Base Completion", "Computer Vision", "Document Classification"], "datasets": ["DBpedia"], "queryEntities": {}}, {"id": "6db044b6-95e5-37f5-9f78-1226ff352f2b", "title": "EIGEN: Event Influence GENeration using Pre-trained Language Models", "score": 0.0, "url": "https://arxiv.org/abs/2010.11764", "pdfUrl": "https://arxiv.org/pdf/2010.11764.pdf", "bibTexUrl": null, "authors": ["Aman Madaan", "Dheeraj Rajagopal", "Yiming Yang", "Abhilasha Ravichander", "Eduard Hovy", "Shrimai Prabhumoye"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Reasoning about events and tracking their influences is fundamental to understanding processes. In this paper, we present EIGEN - a method to leverage pre-trained language models to generate event influences conditioned on a context, nature of their influence, and the distance in a reasoning chain. We also derive a new dataset for research and evaluation of methods for event influence generation. EIGEN outperforms strong baselines both in terms of automated evaluation metrics (by 10 ROUGE points) and human judgments on closeness to reference and relevance of generations. Furthermore, we show that the event influences generated by EIGEN improve the performance on a \"what-if\" Question Answering (WIQA) benchmark (over 3% F1), especially for questions that require background knowledge and multi-hop reasoning.", "startOffset": 0, "endOffset": 816}], "summary": false}, {"title": "Introduction", "figure": 0, "fragments": [{"end_offset": 114, "text": "Humans are adept at anticipating and reasoning about events and their causal effects on other events.", "start_offset": 0}, {"end_offset": 888, "text": "Hence, reasoning about events and influences remains a significant challenge for machines. Understanding such events and tracing their influence chains is essential for end tasks like question answering , process tracking , reasoning about qualitative relationships , and physical commonsense reasoning .", "start_offset": 479}, {"end_offset": 1225, "text": "Previous approaches have studied event understanding in the context of event extraction , temporal event reasoning , and QuestionAnswering .", "start_offset": 933}, {"end_offset": 2312, "text": "Meanwhile, promising evidence from recent work attests to the ability of pretrained language models to encode a wide-range of knowledge from their pretraining corpus , enabling their successful adaptation in downstream tasks . Motivated by these successes, we investigate whether we can adapt PLM for the novel task of event influence generation and determine empirically whether the generated event influences lead to downstream performance gains. Such an exploration entails two major challenges: i) lack of large-scale stand-alone datasets to study event influences, and ii) a framework to leverage PLM to adapt them for event influence generation.", "start_offset": 1530}, {"end_offset": 2740, "text": "Next, we propose our framework, EIGEN, that takes a context and an event, and generates its influences both in forward and backward directions.", "start_offset": 2597}, {"end_offset": 3348, "text": "EIGEN fine-tunes a PLM to generate novel event influences for unseen contexts using masked language modeling.", "start_offset": 3239}, {"end_offset": 3858, "text": "In one such instance, we show how the event influences generated from EIGEN can be easily augmented to a downstream QA task and improve its performance without any need for modifying the underlying model architecture.", "start_offset": 3641}], "summary": true}, {"title": "Related Work", "figure": null, "fragments": [{"end_offset": 627, "text": "Event Influences: There has been immense interest in understanding event chains in stories and news corpora in both unsupervised and supervised settings. Such approaches aim to extract the event chains that are explicitly mentioned in the input text and are unyielding towards implicit event reasoning. Events and their influences have also been studied in restricted domains such as cooking recipes , and in general procedural text as a classification task over a restricted set of events.", "start_offset": 0}, {"end_offset": 1553, "text": "Albeit being resourceful, restricted task formulation limits use of these datasets to adapt for event influence generation task. To \novercome this challenge, we derive a large-scale event-influence dataset from WIQA . Language Models for Knowledge Generation: The use of large scale neural networks to generate knowledge has been studied under various task settings. Sap et al. use LSTM-based encoder-decoder architectures to generate generalpurpose social commonsense knowledge.", "start_offset": 971}, {"end_offset": 1842, "text": "Bosselut et al. proposed COMET, which fine-tunes GPT on ATOMIC and conceptnet for knowledge-completion task.", "start_offset": 1664}, {"end_offset": 2078, "text": "Similar to Bosselut et al, we leverage pre-trained language models for the conditional generation of events.", "start_offset": 1962}, {"end_offset": 2485, "text": "Additionally, unlike COMET, a dataset that can be used for our task is not readily available, and hence we outline a method for adapting existing datasets for our task as an additional contribution.", "start_offset": 2287}], "summary": true}, {"title": "Event Influence Generation", "figure": 1, "fragments": [{"end_offset": 408, "text": "EIGEN is a framework for generating fine-grained event influences for a given context, conditioned on the relation and the hop information. EIGEN leverages a pretrained language model to learn to generate novel event influences over multiple hops. In this section, we present our task formulation , the dataset collection process and (iii) the learning procedure .", "start_offset": 0}, {"end_offset": 1495, "text": "In this example, for the relation hurt-by, and a hop-length h = 1, we aim to generate the text cloudy skies .", "start_offset": 1382}, {"end_offset": 1784, "text": "Lack of datasets remains a challenge for studying the task of event influence generation.", "start_offset": 1695}, {"end_offset": 2177, "text": "The influence graph captures the interactions between the events and their influences and external perturbations in the context of the process described by the passage.", "start_offset": 2009}, {"end_offset": 2866, "text": "For creating multi-hop training samples for our task, we exploit the transitive compositionality of the influence relations.", "start_offset": 2742}, {"end_offset": 3967, "text": "In our dataset, each procedural passage is used to create multiple training examples from variations in ns, ri, hi.", "start_offset": 3852}, {"end_offset": 4293, "text": "EIGEN uses a language model to estimate the probability of generating an end node nt for an input xi.", "start_offset": 4192}, {"end_offset": 5207, "text": "Each transformer block consists of two operations: a masked version of the multi-headed self-attention followed by a feed-forward network .", "start_offset": 5040}], "summary": true}, {"title": "Experiments", "figure": 3, "fragments": [{"end_offset": 341, "text": "The dataset statistics by relation type and hop information are shown in Table 1. EIGEN is based on the GPT-2 implementation by Wolf et al.3, and uses nucleus sampling for decoding output sequences over the fine-tuned language model.", "start_offset": 76}, {"end_offset": 588, "text": "All of our experiments were done on a single Nvidia GeForce RTX 2080 Ti.", "start_offset": 516}, {"end_offset": 888, "text": "LSTM Seq-to-Seq: We train an LSTM based sequence to sequence model which uses \nglobal attention described in (Luong et al, 2015).", "start_offset": 651}, {"end_offset": 1235, "text": "The goal of this baseline is to understand the extent to which GPT-2 without fine-tuning could encode event influence information.5.", "start_offset": 1104}, {"end_offset": 3133, "text": "EIGEN outperforms COMET in all the metrics by a considerable margin, (by about 8 ROUGE points), reinforcing the importance of generating knowledge that is grounded in context.", "start_offset": 2958}, {"end_offset": 3617, "text": "Table 3 also highlights that the 3-hop generations score higher than the 2-hop relations for help and hurts relations.", "start_offset": 3499}, {"end_offset": 6857, "text": "We found that for 20% of the cases, the generated target event was correct but was expressed differently compared to the reference text class in Table 5).", "start_offset": 6678}, {"end_offset": 7193, "text": "In 5% of the samples , the model generates events with opposite polarity compared to the reference.", "start_offset": 7084}, {"end_offset": 7406, "text": "Table 6 shows the ablation results by removing each of paragraph, reverse edges and hop information from the 4-tuple xi = .", "start_offset": 7259}], "summary": true}, {"title": "Downstream QA", "figure": 8, "fragments": [{"end_offset": 127, "text": "In this section, we examine the utility of EIGENgenerated graphs in a downstream question answering task on the WIQA benchmark.", "start_offset": 0}, {"end_offset": 431, "text": "Specifically, each question in WIQA consists of a context paragraph P and two input events nc and ne.", "start_offset": 330}, {"end_offset": 911, "text": "We use EIGEN to augment the event influences in each sample in the QA task as additional context. Concretely, for the given context P , and the event influences nc and ne, we generate forward influences for nc and reverse influences for ne using EIGEN. This scheme is intended to generate reasoning chains that connect nc to ne, even if ne is not an immediate consequence of nc.", "start_offset": 533}, {"end_offset": 2001, "text": "Tables 7, 8, and 9 show the results from our experiments on the WIQA QA dataset.", "start_offset": 1921}, {"end_offset": 2906, "text": "Further, Tables 8 and 9 show the accuracy of our method vs. the vanilla BERT model by question type and number of hops between nc and ne. We observe from Table 8 that augmenting the context with generated influences from EIGEN leads to considerable gains over BERT based model, with the largest improvement seen in 3-hop questions. The strong performance on the 3-hop question supports our hypothesis that generated influences might be able to connect two event influences that are farther apart in the reasoning\nchain. We also show in Table 9 that augmenting with EIGEN improves performance on the difficult exogenous category of questions, which requires background knowledge.", "start_offset": 2228}], "summary": true}, {"title": "Conclusion", "figure": null, "fragments": [{"end_offset": 896, "text": "We define the problem of event-influence reasoning as a generation task conditioned on context, particularly exploring the efficacy of large scale pre-trained language models for the task. We use human-curated event influence graphs to train a model to generate targeted event influences grounded in a context. Our experiments with ablations and error analysis provide insights into how to effectively adapt pretrained language models for event influence generation and opens up exciting avenues for further research. Our method outperforms strong baselines on both automated and human evaluations. Furthermore, generated influences improve performance on the benchmark\nWIQA QA task without architectural changes to the model. Future work would extend the generalizability of this method to understand more complex and volatile event influences, such as events in news articles and stock markets.", "start_offset": 0}, {"end_offset": 1338, "text": "This material is based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200, and in part by grants from National Science Foundation Secure and Trustworthy Computing program . The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.", "start_offset": 914}], "summary": true}, {"title": "A Appendix", "figure": null, "fragments": [{"end_offset": 1067, "text": "All the dropouts were set to 0.1 We found the best hyperparameter settings by searching the space using the following hyperparameters.", "start_offset": 907}], "summary": true}], "figures": [{"id": "arXiv_pdf_2010_v2_0059@2010.11764-Figure-0.jpg", "caption": "Figure 1: An overview of our methodology. The procedural text describes the process of photosynthesis. For this example, we generate the influence graph for the event more sunlight. The influence graph is generated for the relation types - helps, hurt by, helped by and hops = {1, 2}. A sample output influence graph shows the generated events - bright skies, cloudy skies, plants trap sunlight, and plants grow taller"}, {"id": "arXiv_pdf_2010_v2_0059@2010.11764-Table-7.jpg", "caption": "Table 1: Breakdown of number of samples by relation type, distance, and split. We maintain the same traindev-test split as the WIQA dataset."}, {"id": "arXiv_pdf_2010_v2_0059@2010.11764-Table-8.jpg", "caption": "Table 2: Generation Quality for EIGEN and the baseline. BLEU-n refers to geometric average of BLEU scores calculated upto n-grams. *- indicates that this baseline model is not fine-tuned on our dataset."}, {"id": "arXiv_pdf_2010_v2_0059@2010.11764-Table-1.jpg", "caption": "Table 3: Generation Quality of EIGEN by Relation Type and Node Distance"}, {"id": "arXiv_pdf_2010_v2_0059@2010.11764-Table-0.jpg", "caption": "Table 4: Results of human evaluation. The numbers show the percentage(%) of times a particular option was selected for each metric."}, {"id": "arXiv_pdf_2010_v2_0059@2010.11764-Table-2.jpg", "caption": "Table 5: Examples of error categories. Error analysis is only shown for the incorrect outputs."}, {"id": "arXiv_pdf_2010_v2_0059@2010.11764-Table-3.jpg", "caption": "Table 6: Ablation experiments to understand the contribution of each of paragraph, reverse edges and hop information to the generation of the target event."}, {"id": "arXiv_pdf_2010_v2_0059@2010.11764-Table-4.jpg", "caption": "Table 7: QA Accuracy"}, {"id": "arXiv_pdf_2010_v2_0059@2010.11764-Table-5.jpg", "caption": "Table 8: QA accuracy by number of hops"}, {"id": "arXiv_pdf_2010_v2_0059@2010.11764-Table-6.jpg", "caption": "Table 9: QA accuracy by question type"}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Question Answering", "Knowledge Base Completion"], "datasets": [], "queryEntities": {}}, {"id": "d965c6a3-53af-3d10-9a91-3238ccd309d0", "title": "Neural Language Modeling for Contextualized Temporal Graph Generation", "score": 0.0, "url": "https://arxiv.org/abs/2010.10077", "pdfUrl": "https://arxiv.org/pdf/2010.10077.pdf", "bibTexUrl": null, "authors": ["Aman Madaan", "Yiming Yang"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "This paper presents the first study on using large-scale pre-trained language models for automated generation of an event-level temporal graph for a document. Despite the huge success of neural pre-training methods in NLP tasks, its potential for temporal reasoning over event graphs has not been sufficiently explored. Part of the reason is the difficulty in obtaining large training corpora with human-annotated events and temporal links. We address this challenge by using existing IE/NLP tools to automatically generate a large quantity (89,000) of system-produced document-graph pairs, and propose a novel formulation of the contextualized graph generation problem as a sequence-to-sequence mapping task. These strategies enable us to leverage and fine-tune pre-trained language models on the system-induced training data for the graph generation task....", "startOffset": 0, "endOffset": 860}], "summary": false}, {"title": "Introduction", "figure": 0, "fragments": [{"end_offset": 155, "text": "Temporal reasoning is crucial for analyzing the interactions among complex events and producing coherent interpretations of text data .", "start_offset": 0}, {"end_offset": 1578, "text": "These methods focus on rule-based and statistical submodules to extract verb-centered events and the temporal relations among them. As an emerging area of NLP, large scale pre-trained language models have made strides in addressing challenging tasks like commonsense knowledge graph completion and task-oriented dialog generation .", "start_offset": 1192}, {"end_offset": 1920, "text": "This paper focuses on the problem of generation of an event-level temporal graph for each document, and we refer to this task as contextualized graph generation.", "start_offset": 1759}, {"end_offset": 2481, "text": "We also address a related open challenge, which is a prerequisite to our main goal: the difficulty of obtaining a large quantity of training graphs with human-annotated events and temporal relations.", "start_offset": 2282}, {"end_offset": 3127, "text": "We then encode the graph in each training pair as a string in the graph representation format DOT, transforming the text-to-graph mapping into sequence-to-sequence mapping. We fine-tune GPT-2 on this dataset of document-graph pairs, which yields large performance gains over strong baselines on system generated test set and outperforms CAEVO on TimeBankDense on multiple metrics.", "start_offset": 2724}, {"end_offset": 3963, "text": "We present the first investigation on using\nlarge pre-trained language models for contextualized temporal event graph generation by proposing a new formulation of the problem as a sequence-to-sequence mapping task. 2. We address the difficulty of obtaining a large\ncollection of human-annotated graphs, which is crucial for effective fine-tuning of pretrained models, by automatically producing a collection of 89,000 document-graph pairs. 3. Our experimental results on both the systemgenerated test set and a hand-labeled, out-of-domain dataset , show the advantage of our proposed approach over strong baselines.", "start_offset": 3258}], "summary": true}, {"title": "Related Work", "figure": null, "fragments": [{"end_offset": 538, "text": "Notable systems developed in response include CAEVO , followed by the more recent Cogcomptime . Both CAEVO and Cogcomptime use several statistical and rule-based methods like event extractors, dependency parsers, semantic role labelers, and time expression identifiers for the task.", "start_offset": 214}, {"end_offset": 1471, "text": "We use CAEVO over Cogcomptime to generate a large-scale corpus for our task and to evaluate our system.", "start_offset": 1368}, {"end_offset": 2508, "text": "We note that the problem of temporal graph extraction is different from the more popular task of Temporal relation extraction , which deals with classifying the temporal link between two already extracted events. State of the art Temprel systems use neural methods , but typically use a handful of documents for their development and evaluation. Vashishtha et al. are a notable exception by using Amazon Mechanical Turks to obtain manual annotations over a larger dataset of 16,000 sentences.", "start_offset": 1929}, {"end_offset": 2981, "text": "Language Models for Graph Generation The task of generating graphs using language models has gained a lot of attention. Recently, Bosselut et al. proposed COMET, a system that finetunes GPT on commonsense knowledge graphs like ATOMIC and conceptnet for commonsense kb completion.", "start_offset": 2632}, {"end_offset": 3715, "text": "Similar to their work, we formulate graph generation as an auto-regressive task.", "start_offset": 3635}, {"end_offset": 4267, "text": "While our approach does not rely on any particular language model, it would be interesting to see the gains achieved by the much larger GPT-3 on the dataset produced by our method.2.", "start_offset": 4064}], "summary": true}, {"title": "Deriving Large-scale Dataset for the Temporal Graph Generation", "figure": 10, "fragments": [{"end_offset": 973, "text": "Given a query consisting of Cr, r, and ep, generate eq. Task 2: Temporal Graph Generation: Given a document D, generate the corresponding temporal graph G .\nFigure 1 illustrates the two tasks. Task 1 is similar to knowledge base completion, with a difference that the output events eq are generated, and not drawn from a fixed set of events.", "start_offset": 628}, {"end_offset": 2346, "text": "For each document D, we use CAEVO to extract the dense temporal graph consisting of i) the set of verbs, and ii) the set of temporal relations between the extracted verbs. CAEVO extracts six temporal relations: before, after, includes, is included, simultaneous, and vague.", "start_offset": 2049}, {"end_offset": 4719, "text": "Creating Sub-graphs using Event Communities: We discovered that the graph generated for a given document typically has several sub-graphs that are either completely disconnected or have high intra-link density.", "start_offset": 4500}, {"end_offset": 5251, "text": "Instead, we propose a novel approach based on the detection of event communities to divide a graph into sub-graphs, such that the events in a sub-graph are more densely connected to each other.", "start_offset": 5058}, {"end_offset": 6220, "text": "We retain examples with multiple true eq in the training data because they help the model learn diverse temporal patterns that connect two events.", "start_offset": 6074}, {"end_offset": 6742, "text": "DOT supports a wide variety of graphs and allows augmenting graphs with node, edge, and graph level information.", "start_offset": 6630}, {"end_offset": 7092, "text": "The edges are listed in the order in which their constituent nodes appear in the document.", "start_offset": 7002}], "summary": true}, {"title": "Model", "figure": null, "fragments": [{"end_offset": 153, "text": "We train a conditional language model to solve both the tasks.", "start_offset": 80}, {"end_offset": 999, "text": "To circumvent this, we train our model by masking the loss terms corresponding to the input xi, similar to Bosselut et al.", "start_offset": 869}, {"end_offset": 2059, "text": "Each transformer block consists of two operations: an auto-regressive version of the multiheaded self-attention operation followed by a feed-forward operation. Both these operations are surrounded by layer normalization and a residual connection .", "start_offset": 1731}], "summary": true}, {"title": "Experiments and Results", "figure": 11, "fragments": [{"end_offset": 360, "text": "We evaluate our method on two different datasets: i) Test splits of our dataset , and ii) A hand-labeled mixed-domain corpus derived from TimeBank-Dense . We adapt TB-Dense for our task by applying the same pre-processing operations as were used for TG-Gen, and only use the test splits of TimeBank-Dense for evaluation.", "start_offset": 0}, {"end_offset": 1069, "text": "The models were fine-tuned for five epochs, with a runtime of 48 hours/epoch for Task 1 and 52 hours/epoch for Task 2. The details on hyperparameter tuning are deferred to the Appendix.", "start_offset": 884}, {"end_offset": 3107, "text": "Metrics Given a query , with Cr being the context and ep as the source event, Task 1 is to generate an event eq such that r .", "start_offset": 2895}, {"end_offset": 4388, "text": "We found that 53% of the errors were caused by GPT-2 generating a non-salient event,\nnchambers/caevo 7Implementation by from https: //github.com/Maluuba/nlg-eval.", "start_offset": 4205}, {"end_offset": 7098, "text": "The difference between the performances of LSTM and GPT-2 is stark, showing that GPT-2 is able to leverage its massive architecture and the powerful self-attention mechanism for the challenging task of graph generation.", "start_offset": 6879}, {"end_offset": 7593, "text": "Both the systems achieve high parsing rates , with GPT-2 generating valid DOT files 94.6% of the time.", "start_offset": 7484}, {"end_offset": 8887, "text": "Results on TB-Dense The results on TB-Dense in Table 5 show that despite mismatched domain, GPT-2 performs strongly across a wide range of metrics, including BLEU, ROUGE, GED, ISO, and temporal awareness. Comparing the node-set metrics, we see that GPT-2 leads CAEVO by over eight precision points , but loses on recall as CAEVO extracts nearly every verb in the document as a potential event.", "start_offset": 8484}, {"end_offset": 9247, "text": "We also observe that edge extraction is highly sensitive to node extraction ; for GPT-2, a 27% drop in vF1 causes a 68% drop in eF1 .", "start_offset": 9025}], "summary": true}, {"title": "Conclusion and Future Work", "figure": null, "fragments": [{"end_offset": 124, "text": "Current methods for generating event-level temporal graphs are developed with relatively small amounts of hand-labeled data.", "start_offset": 0}, {"end_offset": 1062, "text": "Our experiments strongly support the effectiveness of the proposed approach, which significantly outperforms strong baselines, including traditional IE techniques. We plan to explore techniques for adapting large-scale language models on unseen domains and at multiple granularity levels in the future.", "start_offset": 760}, {"end_offset": 1355, "text": "This material is based on research sponsored in part by the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.", "start_offset": 1080}], "summary": true}, {"title": "A Learning Event Communities Using Community Detection", "figure": null, "fragments": [{"end_offset": 96, "text": "In this section, we provide the details on the community detection algorithm used by our method.", "start_offset": 0}, {"end_offset": 765, "text": "We use the undirected link between two events ej , ei as a proxy for them being co-referential, and learn temporal event communities utilizing the concept of modularity, first introduced by . Formally, let A be the undirected adjacency matrix for a temporal graph G such that A = 1 if ei and ej are connected by a temporal relation, and 0 otherwise.", "start_offset": 378}, {"end_offset": 1819, "text": "A high Q would promote fsame > frandom and thereby encourage highly inter-connected event communities.", "start_offset": 1717}, {"end_offset": 2090, "text": "We use the fast implementation provided by for calculating event communities iteratively.", "start_offset": 1978}, {"end_offset": 2289, "text": "We use a similar approximation at test time: given a document D, we first break it down into sub-documents using CAEVO and then feed each sub-document to our method.", "start_offset": 2124}], "summary": true}, {"title": "B Dataset Statistics", "figure": null, "fragments": [{"end_offset": 76, "text": "Tables 8, 9, and 10 list various statistics calculated from the source data.", "start_offset": 0}], "summary": true}, {"title": "C Examples", "figure": null, "fragments": [{"end_offset": 155, "text": "Figures 4-9 show randomly picked examples from the test corpus. Each figure shows the text, the corresponding true graph, and the graph predicted by GPT-2.", "start_offset": 0}], "summary": true}], "figures": [{"id": "arXiv_pdf_2010_v2_0051@2010.10077-Figure-2.jpg", "caption": "Figure 1: Task overview: given a document (left), automatically extract a temporal graph (right)."}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Figure-7.jpg", "caption": "Figure 2: Event temporal graph and the extracted communities for a sample document. Each community is shown in different color. The singleton nodes (gray) are dropped. The nodes are only annotated with the verbs for brevity. The edge labels and directions are not used for community detection."}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Figure-6.jpg", "caption": "Figure 3: Temporal graph and the corresponding DOT representation for the sentence: In the southeastern city of Trebisov, Roma clashed fiercely with the police, leading to arrests in which Roma activists said excessive force was used."}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Figure-5.jpg", "caption": "Figure 4"}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Figure-3.jpg", "caption": "Figure 5"}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Figure-4.jpg", "caption": "Figure 6"}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Figure-1.jpg", "caption": "Figure 7"}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Figure-0.jpg", "caption": "Figure 8"}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Figure-8.jpg", "caption": "Figure 9"}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Table-2.jpg", "caption": "Table 1: Effect of pruning operations on the number of relations and events."}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Table-6.jpg", "caption": "Table 2: Dataset statistics."}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Table-1.jpg", "caption": "Table 4: Task 1 results. Bi is the BLEU score calculated with i-grams."}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Table-0.jpg", "caption": "Table 6: Average number of vertices (|V|), edges (|E|), and degree (deg(V)). GPT-2 produces graphs that are closest to the true graphs with respect to all three metrics."}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Table-5.jpg", "caption": "Table 7: Verbs in GPT-2 generated graphs."}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Table-3.jpg", "caption": "Table 8: Top Descriptors for the filtered Dataset. Note that each article is typically assigned more than one descriptor."}, {"id": "arXiv_pdf_2010_v2_0051@2010.10077-Table-4.jpg", "caption": "Table 9: Most frequent events extracted by CAEVO."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Knowledge Graph Completion", "Relation Extraction", "Question Answering", "Knowledge Base Completion"], "datasets": [], "queryEntities": {}}, {"id": "caece7e2-5686-3230-9a23-d278912685ea", "title": "BERTnesia: Investigating the capture and forgetting of knowledge in BERT", "score": 0.0, "url": "https://arxiv.org/abs/2010.09313", "pdfUrl": "https://arxiv.org/pdf/2010.09313.pdf", "bibTexUrl": null, "authors": ["Jonas Wallat", "Jaspreet Singh", "Avishek Anand"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "Probing complex language models has recently revealed several insights into linguistic and semantic patterns found in the learned representations. In this paper, we probe BERT specifically to understand and measure the relational knowledge it captures. We utilize knowledge base completion tasks to probe every layer of pre-trained as well as fine-tuned BERT (ranking, question answering, NER). Our findings show that knowledge is not just contained in BERT's final layers. Intermediate layers contribute a significant amount (17-60%) to the total knowledge found. Probing intermediate layers also reveals how different types of knowledge emerge at varying rates. When BERT is fine-tuned, relational knowledge is forgotten but the extent of forgetting is impacted by the fine-tuning objective but not the size of the dataset....", "startOffset": 0, "endOffset": 828}], "summary": false}, {"title": "Introduction", "figure": null, "fragments": [{"end_offset": 584, "text": "BERT has over 100 million parameters and essentially trades off transparency and interpretability for performance. Loosely speaking, probing is a commonly used technique to better understand the inner workings of BERT and other complex language models .", "start_offset": 285}, {"end_offset": 1799, "text": "While a large body of work exists on probing BERT for linguistic patterns and semantics, there is limited work on probing these models for the factual and relational knowledge they store. Recently, Petroni et al. probed BERT and other language models for relational knowledge in order to determine the potential of using language models as automatic knowledge bases. Their approach converted queries in the knowledge base completion task of predicting arguments or relations from a KB triple into a natural language cloze task, e.g, is the president of the USA. This is done to make the query compatible with the pre-training masked language modeling objective. They consequently showed that a reasonable amount of knowledge is captured in BERT by considering multiple relation probes.", "start_offset": 946}, {"end_offset": 2482, "text": "While this type of layer-by-layer probing has been conducted for syntactic, grammatical, and semantic patterns; knowledge probing has only been conducted on final layer representations.", "start_offset": 2297}, {"end_offset": 2838, "text": "Furthermore, we explore how knowledge is impacted when fine-tuning on knowledge-intensive tasks such as question answering and ranking.", "start_offset": 2703}, {"end_offset": 4018, "text": "When the size of the dataset is fixed and training objective varies, the ranking model forgets less than the QA model.", "start_offset": 3866}], "summary": true}, {"title": "Related Work", "figure": null, "fragments": [{"end_offset": 548, "text": "In this section, we survey previous work on probing language models with a particular focus on contextual embeddings learned by BERT. Probes have been designed for both static and contextualized word representations. Static embeddings refer to non-contextual embeddings such as GloVe . For the static case, the reader can refer to this survey by Belinkov and Glass . Now we detail probing tasks for contextualized embeddings from language models. Initial work on probing dealt with linguistic pattern detection.", "start_offset": 0}, {"end_offset": 1278, "text": "McCoy et al; Goldberg found that BERT is able to effectively learn syntactic heuristics with natural language inference specific probes. Tenney et al; Liu et al; Jawahar et al. investigated BERT layer-bylayer for various syntactic and semantic patterns like part-of-speech, named entity recognition, coreference resolution, entity type prediction, semantic role labeling, etc.", "start_offset": 864}, {"end_offset": 2542, "text": "Most recently, Petroni et al. found that LMs like BERT can be directly used for the task of knowledge base completion since they are able to memorize more facts than some automatic knowledge bases.", "start_offset": 2338}, {"end_offset": 3629, "text": "In terms of experiments, we do not focus on knowledge containment in different language models, rather focus on investigating how knowledge emerges specifically in BERT.", "start_offset": 3460}], "summary": true}, {"title": "Experimental Setup", "figure": 18, "fragments": [{"end_offset": 527, "text": "BERT is a bidirectional text encoder built by stacking several transformer layers. BERT is often pre-trained with two tasks: next sentence classification and masked language modeling . MLM is cast as a classification task over all tokens in the vocabulary. It is realized by training a decoder that takes as input the mask token embedding and outputs a probability distribution over vocabulary tokens. In our experiments we used BERT base pretrained on the BooksCorpus and English Wikipedia.", "start_offset": 0}, {"end_offset": 926, "text": "The following is a list of all fine-tuned models used in our experiments:\n1. NER-CONLL: named entity recognition model tuned on Conll-2003 . 2. QA-SQUAD-1: A question answering\nmodel trained on SQuAD\n1 .", "start_offset": 649}, {"end_offset": 4969, "text": "Measuring Knowledge We convert the probability distribution output of the decoding head to a ranking with the most probable token at rank 1.", "start_offset": 4829}, {"end_offset": 6030, "text": "Caveats of probing with cloze statements: Note that BERT, MLM-MSMARCO, and MLMSQUAD are trained for the task of masked word prediction which is exactly the same task as our probes.", "start_offset": 5850}, {"end_offset": 6303, "text": "Hence, good performance in our probes at the last layers for MLM models can be partially attributed to task-based knowledge.", "start_offset": 6179}], "summary": true}, {"title": "Results", "figure": 16, "fragments": [{"end_offset": 200, "text": "In contrast to existing work, we want to analyze relation knowledge across layers to measure the total knowledge contained in BERT and observe\nthe evolution of relational knowledge through the layers.", "start_offset": 0}, {"end_offset": 778, "text": "It is immediately evident that a significant amount of knowledge is stored in the intermediate layers. While the last layer does contain a reasonable amount of knowledge, a considerable proportion of relations seem to be forgotten and the intermediate layers contain relational knowledge that is absent in the final layer.", "start_offset": 456}, {"end_offset": 1062, "text": "For instance, the answer to Rocky Balboa was born in [MASK] is correctly predicted as Philadelphia by Layer 10 whereas the rank of Philadelphia in the last layer drops to 26 for BERT.", "start_offset": 879}, {"end_offset": 1493, "text": "We also measured the fraction of relationship types in T-REx that are better captured in the intermediary layers .", "start_offset": 1370}, {"end_offset": 2983, "text": "While member of is forgotten in the last layers, the relation diplomatic relation is never learned at all, and official language of is only identifiable in the last two layers.", "start_offset": 2807}, {"end_offset": 4470, "text": "With previous studies indicating that the last layers make way for task-specific knowledge , the ranking model can retain a larger amount of knowledge when compared to other fine-tuning tasks in our experiments.", "start_offset": 4236}, {"end_offset": 6927, "text": "Considering the QA span prediction objective, we first see that the total amount of knowledge stored in QA-SQUAD-2 is higher for 3/4 knowledge probes .", "start_offset": 6755}, {"end_offset": 7337, "text": "This result hints to the fact that a more difficult task on the same dataset forces BERT to remember more relational knowledge in its final layers as compared to the relatively simpler SQUAD1.", "start_offset": 7136}, {"end_offset": 7954, "text": "Figure 8 shows the evolution of knowledge captured for MLM-MSMARCO vs RANK-MSMARCO.", "start_offset": 7871}], "summary": true}, {"title": "Discussion and Conclusion", "figure": 19, "fragments": [{"end_offset": 336, "text": "In this paper, we introduce a framework to probe all layers of BERT for knowledge. We experimented on a variety of probes and fine-tuning tasks and found that BERT contains more knowledge than was reported earlier. Our experiments shed light on the hidden knowledge stored in BERT and also some important implications to model building.", "start_offset": 0}, {"end_offset": 663, "text": "We show that factual knowledge, like syntactic and semantic patterns, is also forgotten at the last layers due to fine-tuning.", "start_offset": 537}, {"end_offset": 1352, "text": "Interestingly, forgetting is not mitigated by larger datasets which potentially contain more factual knowledge . Instead, we find that knowledge-intensive tasks like ranking do mitigate forgetting compared to span prediction. Although the fine-tuned models always contain less knowledge, with significant forgetting in the last layers, RANKMSMARCO remembers relatively more relationship types than BERT in its last layer .", "start_offset": 841}, {"end_offset": 1826, "text": "Essentially, ranking tasks encourage the retention of factual knowledge since they are seemingly required for reasoning between the relative relevance of documents to a query. Our results have direct implications on the use of BERT as a knowledge base.", "start_offset": 1545}, {"end_offset": 2257, "text": "More knowledge-intensive QA models like answer generation models may also show a similar trend as ranking tasks but require investigation.", "start_offset": 2119}], "summary": true}, {"title": "Appendix", "figure": 17, "fragments": [{"end_offset": 442, "text": "This adds a span prediction head to the default BERT, I.e. a linear layer that computes logits for the span start and span end. So for a given question and a context, it classifies the indices in in which the answer starts and ends.", "start_offset": 210}, {"end_offset": 796, "text": "Learning rate was 3e-5, batch size 12, best model after 2 epochs.", "start_offset": 731}, {"end_offset": 1409, "text": "The experiments, however, have been run on a computing cluster with 6 nodes.", "start_offset": 1333}, {"end_offset": 2323, "text": "Subset of the Englisch Wikipedia for long term dependency language modeling.", "start_offset": 2247}, {"end_offset": 3324, "text": "Combines the 100,000+ question answer pairs with 50,000 unanswerable questions.", "start_offset": 3245}, {"end_offset": 3629, "text": "It consists of over 1m queries and the 8,8m passages.", "start_offset": 3576}, {"end_offset": 3817, "text": "For MLM, we appended the queries with all candidate passages before feeding into BERT. Additional precisions for Figure 4 can be found in Figure 9.", "start_offset": 3668}, {"end_offset": 4070, "text": "For comparing MLM and QA on SQuAD , Figure 15 and 16 show more precisions.", "start_offset": 3993}], "summary": true}], "figures": [{"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-13.jpg", "caption": "Figure 10: Mean performance of BERT across all layers and probe sets."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-10.jpg", "caption": "Figure 11: Effect of dataset size. Showing P@10"}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-12.jpg", "caption": "Figure 12: Effect of dataset size. Showing P@100"}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-14.jpg", "caption": "Figure 13: Effect of dataset size. Showing P@10 for the QA objective."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-11.jpg", "caption": "Figure 14: Effect of dataset size. Showing P@100 for the QA objective."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-9.jpg", "caption": "Figure 15: Effect of Fine-Tuning Objective on fixed size data: SQUAD. Showing P@10."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-6.jpg", "caption": "Figure 16: Effect of Fine-Tuning Objective on fixed size data: SQUAD. Showing P@100."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-7.jpg", "caption": "Figure 17: Effect of Fine-Tuning Objective on fixed size data: MSMarco. Showing P@10."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-8.jpg", "caption": "Figure 18: Effect of Fine-Tuning Objective on fixed size data: MSMarco. Showing P@100."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-0.jpg", "caption": "Figure 1: P@k (upper value) vs last layer P@k (lower value) for all models for each LAMA probe."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-3.jpg", "caption": "Figure 2: Mean P@1 of BERT across all layers."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-2.jpg", "caption": "Figure 3: P@1 across all layers for BERT for select relationship types from T-REx."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-1.jpg", "caption": "Figure 4: Knowledge contained per layer measured in terms of P@1 on T-REx."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-4.jpg", "caption": "Figure 5: Effect of dataset size. Mean P@1 across layers for BERT, MLM-MSMARCO and MLM-SQUAD."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-5.jpg", "caption": "Figure 6: Effect of dataset size. Mean P@1 across layers for QA-SQUAD-1 and QA-SQUAD-2."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-17.jpg", "caption": "Figure 7: Effect of Fine-Tuning Objective on fixed size data: SQUAD."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-16.jpg", "caption": "Figure 8: Effect of Fine-Tuning Objective on fixed size data: MSMarco."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Figure-15.jpg", "caption": "Figure 9: Mean performance in different precisions on T-REx sets for BERT, QA-SQUAD-2, RANKMSMARCO, NER-CONLL."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Table-2.jpg", "caption": "Table 1: Knowledge probes used in the experiments. Petroni et al. (2019) subsampled ConceptNet (Speer and Havasi, 2012), T-REx (ElSahar et al., 2018), Google-RE (Orr, 2013) and Squad (Rajpurkar et al., 2016)."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Table-0.jpg", "caption": "Table 2: Fraction of relationship types (of the 41 T-REx) that are forgotten in the last layer. If mean P 12@1 < mean P l@1 for a particular relation type then that relation is considered to be forgotten at the last layer."}, {"id": "arXiv_pdf_2010_v2_0048@2010.09313-Table-1.jpg", "caption": "Table 3: Mean knowledge contained in the last layer (P@1) vs knowledge contained in all layers (P@1) for each probe."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Semantic Role Labeling", "Named Entity Recognition", "Coreference Resolution", "Common Sense", "Machine Translation", "Reading Comprehension", "Natural Language Inference", "Knowledge Base Completion", "Question Answering", "Sentence Classification"], "datasets": ["SQuAD", "WikiText-2", "CoNLL 2003", "ImageNet"], "queryEntities": {}}, {"id": "87d97384-2835-30d8-872a-4c547c73daf3", "title": "On the Complementary Nature of Knowledge Graph Embedding, Fine Grain Entity Types, and Language Modeling", "score": 0.0, "url": "https://arxiv.org/abs/2010.05732", "pdfUrl": "https://arxiv.org/pdf/2010.05732.pdf", "bibTexUrl": null, "authors": ["Rajat Patel", "Francis Ferraro"], "year": 2020, "sections": [{"title": "Abstract", "figure": null, "fragments": [{"text": "We demonstrate the complementary natures of neural knowledge graph embedding, fine-grain entity type prediction, and neural language modeling. We show that a language model-inspired knowledge graph embedding approach yields both improved knowledge graph embeddings and fine-grain entity type representations. Our work also shows that jointly modeling both structured knowledge tuples and language improves both.", "startOffset": 0, "endOffset": 411}], "summary": false}, {"title": "Introduction", "figure": 0, "fragments": [{"end_offset": 805, "text": "Properly making use of this structured knowledge is a prime challenge. Knowledge graph embedding addresses this problem by representing the nodes and their edges in a continuous vector space. Learning these representations deduces new facts from and identifies dubious entries in the knowledge base. It also improves relation extraction , knowledge base completion and entity resolution . Entity typing can provide crucial constraints and information on the knowledge contained in a KG.", "start_offset": 183}, {"end_offset": 1754, "text": "Building off of insights and advances in knowledge graph embedding, entity typing, and language modeling, we identify and advocate for leveraging the complementary nature of knowledge graphs, entity typing, and language modeling.", "start_offset": 1525}, {"end_offset": 2333, "text": "Figure 1 provides an overview of the joint learning framework proposed in this work: an entity along with its relations are represented in a continuous vector space.", "start_offset": 2151}, {"end_offset": 2760, "text": "By using the type and the factual information the framework enhances the comprehension of the focus entity in downstream applications like language modeling.1.", "start_offset": 2602}, {"end_offset": 4438, "text": "It advocates for a languagemodeling based knowledge graph embedding architecture that achieves state-of-the-art performance on knowledge graph completion/fact prediction against comparable methods. It introduces a neural-based technique based on both knowledge graph embedding and language modeling to predict fine-grain entity types, which yields competitive through state-of-the-art performance against comparable methods.", "start_offset": 4010}], "summary": true}, {"title": "Background", "figure": null, "fragments": [{"end_offset": 963, "text": "A vast number of knowledge graph embeddings techniques have been proposed over the years to mirror the entities and relations in the knowledge graphs. RESCAL is one of the first semantic-based embedding technique that captures the latent interaction between the entities and the relation. A model such as RESCAL can use graph properties to improve the underlying entity and relation representations . A more simplified approach is defined in DistMult by restricting the relation matrix to a diagonal matrix. Neural Tensor Network is one such technique that combines the relation specific tensors with head and tail vector representation over non-linear activation function mapped to hidden layer representation.", "start_offset": 113}, {"end_offset": 1257, "text": "TransH overcomes the shortcomings of TransE by modeling the vector representation with relations specific hyperplane.", "start_offset": 1120}, {"end_offset": 2111, "text": "This work also led to an important contribution of a labeled dataset FIGER, widely used as a benchmark dataset in measuring the performance of fine-grain entity type prediction architectures.", "start_offset": 1920}, {"end_offset": 3053, "text": "Zhang et al. introduced a document level context and signifies the importance of mention level attention mechanism along with the sentence-level context in enhancing the performance of fine-grain entity prediction. Xu and Barbosa enhanced neural fine-grain entity typing by penalizing the cross-entropy loss with hierarchical context loss for the fine-grain type prediction.", "start_offset": 2664}, {"end_offset": 3883, "text": "Ahn et al. and Logan IV et al. have more recently leveraged information from a knowledge base to improve language modeling.", "start_offset": 3746}], "summary": true}, {"title": "Methodology", "figure": 1, "fragments": [{"end_offset": 244, "text": "This section introduces the framework for jointly learning knowledge graph embedding , fine grain entity types and language models . It uses a multi-task learning architecture built over baseline architectures for all three tasks.", "start_offset": 0}, {"end_offset": 554, "text": "Fundamentally, our approach relies on appropriate and select parameter sharing across the KGE, ET, and LM tasks in order to learn these models jointly.", "start_offset": 403}, {"end_offset": 903, "text": "The architecture in Figure 2 embeds the factual entities and the relations.", "start_offset": 828}, {"end_offset": 3055, "text": "The input to the feed-forward layer is a learned final cell state representation Cfinal from the bi-LSTM sequence encoder.", "start_offset": 2933}, {"end_offset": 5010, "text": "Recognizing the type of the given entity has been an integral part of tasks like knowledge base completion , question answering and co-reference resolution.", "start_offset": 4833}, {"end_offset": 6402, "text": "Parameters of the architecture are trained to learn both the factual information as well the corresponding entity types.", "start_offset": 6282}, {"end_offset": 9662, "text": "The attention mechanism used here differs from Shimaoka et al. such that in our work the contextual embeddings share the same attention parameters. The features extracted from the mention encoder m and attention weighted context encoder Cr are concatenated to form a learned representation V = concat that is passed to the feed-forward architecture for classification. The feed-forward architecture is a 3-layer neural architecture with a batch normalization layer present between the first and the second layers with a ReLU activation .", "start_offset": 9059}], "summary": true}, {"title": "Experimental Settings", "figure": null, "fragments": [{"end_offset": 834, "text": "The input to the joint learning architectures are the pre-trained GloVe embedding vectors trained on 840 billion words . The parameters of the baseline and the joint learning architecture are learned with Stochastic Gradient Descent and Adam as a learning rate optimizer. The training of the joint learning networks is performed with alternating optimization. The loss functions of the respective tasks are optimized at each alternate epoch/ interval. The hyper-parameters for training these joint architecture are chosen manually for the bestperforming models on validation sets. Data For a direct comparison of the performance as possible, we use previously studied datasets. We evaluate KG triple classification using the standard datasets of WordNet 11 and Freebase 13 .", "start_offset": 0}, {"end_offset": 1367, "text": "While recent work has advocated for examining variants and other derivatives of these datasets such as FB15k-237 and WN18RR , there is a relative lack of previous experimental work on these newer datasets.", "start_offset": 1087}, {"end_offset": 1775, "text": "The OntoNotes dataset used here is a manually curated dataset by Gillick et al, consisting of 89 different entity types.", "start_offset": 1647}, {"end_offset": 2126, "text": "WikiAuto is curated by distant supervision, with Freebase entities and types and sentence descriptions from Wikipedia articles.", "start_offset": 1999}], "summary": true}, {"title": "Results and Discussion", "figure": 3, "fragments": [{"end_offset": 582, "text": "This section presents the results of our basic KGE, entity typing models, and the joint learning architecture and their comparison to previous methods. The models were trained using either a 16GB V100 or 11GB 2080 TI GPU . The proposed knowledge graph embedding architecture is trained for triple classification task: given an input triple xi, predict whether the fact it represents is true or not. Table 1 provides an overview of performance of our architecture in comparison to previously studies approaches, obtained from the corresponding paper.", "start_offset": 0}, {"end_offset": 1157, "text": "These strong results support our hypothesis that language modeling principles can be an effective knowledge graph embedding technique. In examining perrelation performance on both WN11 and FB13, we observed an increase in the lower bound of accuracy results for relationships on both WordNet and\nFreebase, compared to Socher et al.", "start_offset": 818}, {"end_offset": 2644, "text": "Building on the baseline models, the joint model addresses the implicit constraint given in the knowledge graph.", "start_offset": 2521}, {"end_offset": 3091, "text": "The model is trained with combination of FB15K dataset and WikiAuto to learn the both the factual information along with the entity typing structure.", "start_offset": 2942}, {"end_offset": 3492, "text": "The results show the complementary nature\nof learning fine-grain entity types and knowledge graph embedding jointly with steady performances on either task with respect to their baselines.", "start_offset": 3304}, {"end_offset": 4916, "text": "In 6b, we provide results where we replace the bi-LSTM KGE with LSTM LM.\npendently, without any weight sharing, evaluating the LMs on perplexity and KG prediction accuracy .", "start_offset": 4707}], "summary": true}, {"title": "Conclusion", "figure": null, "fragments": [{"end_offset": 728, "text": "This work proposes a joint learning framework for learning real value representations of words, entities, and relations in a shared embedding space. Joint learning of factual representation with contextual understanding shows improvement in the learning of entity types. Learning the language model with knowledge graph embedding simultaneously enhances the performance on both modeling tasks. Our results suggest that language modeling could accelerate the study of schema-free approaches to both KGE and FNER, and strong performance can be obtained with comparatively simpler, resourcestarved language models. This has promising implications for low-resource, and few-shot, and/or domain-specific information extraction needs.", "start_offset": 0}], "summary": true}], "figures": [{"id": "arXiv_pdf_2010_v2_0030@2010.05732-Figure-0.jpg", "caption": "Figure 1: Our joint learning framework learns the representation for the entity \u201cBarack Obama\u2019s\u201d in the same embedding space as that of the given input contextual description, \u201cBarack Obama gave a speech to Congress.\u201d Further, by learning the entity type of \u2018/person/politician\u2019, the model provides a better contextual understanding of the underlying entity."}, {"id": "arXiv_pdf_2010_v2_0030@2010.05732-Figure-1.jpg", "caption": "Figure 2: Knowledge Graph Embedding as language modeling, where triples are \u201ctokenized\u201d into word embeddings and the computed, sequential output states are used to predict triple correctness."}, {"id": "arXiv_pdf_2010_v2_0030@2010.05732-Figure-2.jpg", "caption": "Figure 4: The architecture for joint learning of knowledge graph embedding with language model. We use an LSTM for the LM component, and a bi-LSTM for the KGE component. The LM LSTM and the forward portion of the bi-LSTM are the same, allowing the transfer of knowledge. The architecture takes in as input the whole sentence and the triplet to learn the semantic structure and factual information from the knowledge base."}, {"id": "arXiv_pdf_2010_v2_0030@2010.05732-Table-0.jpg", "caption": "Table 1: Comparison of previous approaches with proposed method on triple classification task."}, {"id": "arXiv_pdf_2010_v2_0030@2010.05732-Table-1.jpg", "caption": "Table 2: The performance of the proposed fine grain entity architecture to previous approaches on FIGER."}, {"id": "arXiv_pdf_2010_v2_0030@2010.05732-Table-3.jpg", "caption": "Table 3: We compare previous techniques on the WIKIAUTO dataset for fine-grain typing. The proposed method outperforms all previous, comparable techniques. While techniques that utilize disambiguation to improve the results on the knowledge attention (e.g., KA + D (KNET) from Xin et al. (2018a)) can yield very modest improvements, e.g., to 77 micro F1, due to the extra information used, those results are not directly comparable to the proposed model."}, {"id": "arXiv_pdf_2010_v2_0030@2010.05732-Table-2.jpg", "caption": "Table 4: We compare previous techniques on WikiMAN dataset for fine-grain entity type classification."}, {"id": "arXiv_pdf_2010_v2_0030@2010.05732-Table-4.jpg", "caption": "Table 5: We show the changes in performance we observe when training joint fine-grain entity type prediction and triple classification models (bottom portion) vs. single-objective models (top portion). Joint training can lead to improvements on both KGE and FNER."}], "source": "Arxiv", "conference": "Arxiv", "venue": "Arxiv", "tasks": ["Knowledge Graph Embeddings", "Entity Resolution", "Knowledge Graph Completion", "Knowledge Graph Embedding", "Common Sense", "Relation Extraction", "Multi-Task Learning", "Knowledge Base Completion", "Question Answering"], "datasets": ["DBpedia", "OntoNotes"], "queryEntities": {}}]